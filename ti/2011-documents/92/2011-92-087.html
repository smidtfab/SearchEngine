<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" xmlns:fb="http://www.facebook.com/2008/fbml">
<head>
	<title>How reliable are annotations via crowdsourcing: a study about inter-annotator agreement for multi-label image annotation | Mendeley</title>
	<meta http-equiv="Content-Type" content="text/html; charset=UTF-8" />
	<meta name="description" content="(2010) Nowak, Rüger. Proceedings of the international conference on Multimedia information retrieval MIR 10. Read by researchers in: 77% Computer and Information Science, 15% Business Administration. The creation of golden standard datasets is a costly business. Optimally more than one judgment per document is obtained to ensure a high quality on annotations. In this context, we explore how..." />
	<meta name="keywords" content="academic software, research paper, bibliography, digital library, research tool, researcher, academics, library management, reference software, library software, academic research" />
	<meta name="ROBOTS" content="ALL" />
	<meta name="verify-v1" content="KIQZp9vFHmQQVeTanWS2u+xvEpl3D8dHq3MBWpWhX2k=" />
	<meta name="y_key" content="a33e384681c650e5" />
	<meta name="msvalidate.01" content="38901ADA50F068F6162998FE344AE1B2" />
		<meta property="fb:app_id" content="10150110947375581" />
				<meta name="dc.format" content="text/html" />
		<meta name="citation_abstract_html_url" content="http://www.mendeley.com/research/how-reliable-are-annotations-via-crowdsourcing-a-study-about-interannotator-agreement-for-multilabel-image-annotation/" />
		<meta name="citation_isbn" content="9781605588155" />
		<meta name="citation_doi" content="10.1145/1743384.1743478" />
		<meta name="dc.identifier" content="doi:10.1145/1743384.1743478" />
		<meta name="citation_firstpage" content="557" />
		<meta name="prism.startingPage" content="557" />
		<meta name="citation_lastpage" content="566" />
		<meta name="prism.endingPage" content="566" />
		<meta name="citation_journal_title" content="Proceedings of the international conference on Multimedia information retrieval MIR 10" />
		<meta name="prism.publicationName" content="Proceedings of the international conference on Multimedia information retrieval MIR 10" />
		<meta name="citation_publisher" content="ACM Press" />
		<meta name="dc.publisher" content="ACM Press" />
		<meta name="dc.creator" content="S Nowak" />
		<meta name="dc.creator" content="S Rüger" />
		<meta name="citation_authors" content="Nowak, S; Rüger, S" />
		<meta name="citation_title" content="How reliable are annotations via crowdsourcing: a study about inter-annotator agreement for multi-label image annotation" />
		<meta name="dc.title" content="How reliable are annotations via crowdsourcing: a study about inter-annotator agreement for multi-label image annotation" />
		<meta name="citation_date" content="2010" />
		<meta name="dc.date" content="2010" />
		<meta name="prism.publicationDate" content="2010" />
				<meta property="og:site_name" content="Mendeley Research Networks" />
		<meta property="og:image" content="http://www.mendeley.com/graphics/commonnew/facebook_wall_4838362315632650.png" />
		<meta property="og:title" content="How reliable are annotations via crowdsourcing: a study about inter-annotator agreement for multi-label image annotation" />
	
	
	
	
	<script type="text/javascript" language="JavaScript">
	//<![CDATA[
		var hroot = 'http://www.mendeley.com';
		var https_url = 'https://www.mendeley.com';
		var domain = '';
		var page = 'catalog/canonical_view';
		var gmap_key = 'ABQIAAAA4lY5I_eP_7SMch77X9ceKxTWbqW3oODjNEH4JLEC2adgENl52BSqKM2qEk-1DPEWobdnJA0prcCCuA';
		var mendeley_profilename = '';
		var css_bundle_version = '3177468115521681';
				var MendeleyLoadGoogleMaps = false;
			//]]>
	</script>

	<link rel="stylesheet" type="text/css" href="http://www.mendeley.com/min.php/css_3177468115521681" />

    <link rel="stylesheet" type="text/css" href="http://www.mendeley.com/min.php/loggedoutcss_2593637321776965" />









<!--[if lt IE 9]><link href="http://www.mendeley.com/styles/stylesheet-ie_3400696702429153.css" rel="stylesheet" type="text/css" /><![endif]-->
<!--[if IE 6]><link href="http://www.mendeley.com/styles/stylesheet-ie6_4582022760948529.css" rel="stylesheet" type="text/css" /><![endif]-->
<!--[if IE 7]><link href="http://www.mendeley.com/styles/stylesheet-ie7_3108669964670015.css" rel="stylesheet" type="text/css" /><![endif]-->
	<script src="http://ajax.googleapis.com/ajax/libs/jquery/1.3.2/jquery.min.js" type="text/javascript"></script>
	<!--[if lt IE 9]>
    <script src="http://html5shiv.googlecode.com/svn/trunk/html5.js"></script><![endif]-->

	<script type="text/javascript" src="http://use.typekit.com/muv1pzg.js"></script>
<script type="text/javascript">try{Typekit.load();}catch(e){}</script>
	<script type="text/javascript" src="http://www.mendeley.com/min.php/init_3263616806530481"></script>

	<script type="text/javascript">jQuery(document).ajaxSend(function(e, xhr, options){xhr.setRequestHeader('X-CSRF-Token', jQuery.cookie('csrf_token'))});</script>
	
		
	
	
	
    
        
	
	
	
			<script type="text/javascript" src="http://www.mendeley.com/min.php/catalog_2611418047896254"></script>
	
	
	
			<script type="text/javascript" src="http://www.mendeley.com/min.php/facebook_6844293833887706"></script>
	
	
	
	
	<script type="text/javascript">var _sf_startpt=(new Date()).getTime()</script>

	
	<script type="text/javascript">
		var _gaq = _gaq || [];
		_gaq.push(['_setAccount', 'UA-3874710-1']);
		
		(function() {
			var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;
			ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
			var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);
		})();
		
	</script>

</head>
			    	        	        	    		<body onload=" rshInit();" class="three-col papers">
<!-- livesite-2011033101 from www.mendeley.com via 1.1 http1.www.mendeley.com:80 (squid/2.7.STABLE3). RID: e71dc18a-e59c-49bd-8c26-94c20c5f087e -->

		<div id="wrapper">
			<div id="header-container">
				<a href="http://www.mendeley.com" title="Reference management software"><img id="header-logo" alt="Mendeley reference manager logo" src="http://www.mendeley.com/graphics/commonnew/logo-mendeley_1248201417297118.png" title="Reference management software"/></a>
				
										<div id="header-login">
					<a href="https://www.mendeley.com/join/" rel="nofollow" data-log-click="header/join">Create a new account</a> or
						<span class="haz-drop-down">
							<a href="/login/" class="link-button drop-down" rel="nofollow" id="sign-in-btn" title="Sign in to Mendeley"
							   data-log-click="header/signin">Sign in<span class="button-opener"></span></a>
							<div class="drop-down-pos">
								<div id="sign-in" class="drop-down-area" style="display: none;">
									<form method="post" action="https://www.mendeley.com/login/" id="header-login-form"><input type="hidden" name="csrf_token" value="6d9061a74123e525b2ac4178bc9c281d6e9ac338">
										<label for="email-input">E-mail address</label>
										<input id="email-input" class="textbox" type="text" name="email" />
										<label for="password-input">Password</label>
										<input id="password-input" class="textbox" type="password" name="password" />
											<label class="remember-me"><input name="remember" id="remember-me" type="checkbox" value="rememberme"/>
											Remember me</label>

										<input type="submit" id="sign-in-submit" value="Sign in" class="link-button">
									</form>
									<div id="other-links">
                                        
                                        ...or sign in with <a href="#" onclick="Mendeley.FacebookConnect.login('dropdown'); return false;" class="fb-auth" data-log-click="facebook-signin">Facebook</a>
									</div>
								</div>
							</div>
						</span>
					</div>
												</div>

			<div id="main-menu">
				<div id="global-search" class="search">
										<form action="/research-papers/search/" onsubmit="return Mendeley.Search.checkSearch(this);" method="get">
						<div class="haz-drop-down">
							<a class="link-button drop-down" rel="nofollow" id="search-selections-btn" title="Select search scope"><span class="button-text">Papers</span><span class="button-opener"></span></a>
							<div class="drop-down-pos">
								<div id="search-selections" class="drop-down-area" style="display: none;">
																			<a onclick="Mendeley.Search.searchSelected(this);return false;" href="#" id="search-selection-research-papers">Search papers</a>
																			<a onclick="Mendeley.Search.searchSelected(this);return false;" href="#" id="search-selection-groups">Search groups</a>
																			<a onclick="Mendeley.Search.searchSelected(this);return false;" href="#" id="search-selection-people">Search people</a>
																	</div>
							</div>
						</div>
						<div class="search-box">
							<div id="search-input">
								<input class="left" id="searchQuery"  onfocus="Mendeley.Search.inputFocus('focus', this, 'global');" onblur="Mendeley.Search.inputFocus('blur', this, 'global');" type="text" name="query" value="" />
								<input type="hidden" class="search-label" value="research-papers"/>
								<div class="clear"></div>
							</div>
						</div>
						<button class="search-btn" id="simple-search" type="submit">Search</button>
						<div class="clear"></div>
					</form>
				</div>

				<ul>
					<li class="hidden"><strong>Main Navigation</strong></li>
									<li><a href="http://www.mendeley.com/" id="main_menu_get_mendeley">Get Mendeley</a></li>
									<li class="selected"><a href="http://www.mendeley.com/research-papers/" id="main_menu_papers_selected">Papers</a></li>
									<li><a href="http://www.mendeley.com/groups/" id="main_menu_groups">Groups</a></li>
								</ul>
			</div>
			<div id="main-container">
				<div id="content-container">
					<div id="main-content">
																				
    
<div class="heading">
<div class="padding">

	
    <h1>How reliable are annotations via crowdsourcing: a study about inter-annotator agreement for multi-label image annotation</h1>
    <span class="Z3988" title="ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fmendeley.com%2Fmendeley&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.pages=557-566&amp;rft.atitle=How+reliable+are+annotations+via+crowdsourcing%3A+a+study+about+inter-annotator+agreement+for+multi-label+image+annotation&amp;rft.jtitle=Proceedings+of+the+international+conference+on+Multimedia+information+retrieval+MIR+10&amp;rft.aulast=Nowak&amp;rft.auinit1=S&amp;rft.au=R%C3%BCger%2C+S&amp;rft_id=info%3Adoi%2F10.1145%2F1743384.1743478"></span>        	<div class="authors">by
	<script type="text/javascript">$(document).ready(function() {$('.al-vis').hide();$('.al-hide').show();});</script>
					S Nowak, 							S Rüger				</div>
		<div id="breadcrumb" >
			<a href="http://www.mendeley.com/research-papers/computer-and-information-science/">Computer and Information Science</a>
		<span>&rsaquo;</span> <a href="http://www.mendeley.com/research-papers/computer-and-information-science/information-retrieval">Information Retrieval Papers</a>	</div>

        <div class="action-buttons">
                    
						
									

<style type="text/css">

	.add-to-library-buttons .btn-add { display: inline-block; }
	.add-to-library-buttons .btn-added { display: none; }
	.add-to-library-buttons.inLibrary .btn-add { display: none; }
	.add-to-library-buttons.inLibrary .btn-added { display: inline-block; }

</style>

<span class="add-to-library-buttons ">
	<a href="#added-to-library" class="added-to-library-button link-button primary btn-added"><span class="icon"></span>In your library</a>
	<a href="#add-to-library" class="add-to-library-button link-button primary btn-add" data-log-click="article:add-to-library"
		data-dm-log-click="{&quot;event&quot;:&quot;click&quot;,&quot;page&quot;:&quot;articlePage&quot;,&quot;data[]&quot;:[&quot;AddToLibrary&quot;,&quot;85f868f0-aed9-11df-a235-0024e8453de6&quot;,&quot;0&quot;,&quot;1&quot;]}"><span class="icon icon-save-without-files"></span>Save reference to library</a>
</span>

<script type="text/javascript">
var metadata = {"tags":["*****","qm: accuracy","qm: inter-annotator agreement","qm: kappa statistics","qm: kendall","qm: majority vote","task: image annotation","task: labelling"],"abstract":"The creation of golden standard datasets is a costly business. Optimally more than one judgment per document is obtained to ensure a high quality on annotations. In this context, we explore how much annotations from experts differ from each other, how different sets of annotations influence the ranking of systems and if these annotations can be obtained with a crowdsourcing approach. This study is applied to annotations of images with multiple concepts. A subset of the images employed in the latest ImageCLEF Photo Annotation competition was manually annotated by expert annotators and non-experts with Mechanical Turk. The inter-annotator agreement is computed at an image-based and concept-based level using majority vote, accuracy and kappa statistics. Further, the Kendall and Kolmogorov-Smirnov correlation test is used to compare the ranking of systems regarding different ground-truths and different evaluation measures in a benchmark scenario. Results show that while the agreement between experts and non-experts varies depending on the measure used, its influence on the ranked lists of the systems is rather small. To sum up, the majority vote applied to generate one annotation set out of several opinions, is able to filter noisy judgments of non-experts to some extent. The resulting annotation set is of comparable quality to the annotations of experts.","series":"MIR '10","identifiers":{"isbn":"9781605588155","doi":"10.1145\/1743384.1743478"},"website":"http:\/\/portal.acm.org\/citation.cfm?doid=1743384.1743478","stats":{"academic_status":{"3":2,"7":2,"5":4,"4":1,"11":4},"readers":13,"discipline":{"98":3,"67":1,"107":1,"90":2,"176":1,"63":1,"99":2,"101":2},"country":{"France":2,"United States":1,"Spain":3,"Denmark":1,"Germany":4,"United Kingdom":2}},"pages":"557-566","tags_counts":[3,3,3,3,3,3,3,3],"published_in":"Proceedings of the international conference on Multimedia information retrieval MIR 10","type":"journal","url":"how-reliable-are-annotations-via-crowdsourcing-a-study-about-interannotator-agreement-for-multilabel-image-annotation","publisher":"ACM Press","id":"85f868f0-aed9-11df-a235-0024e8453de6","fingerprint":620404966638274944,"authors":[{"forename":"S","surname":"Nowak"},{"forename":"S","surname":"R\u00fcger"}],"title":"How reliable are annotations via crowdsourcing: a study about inter-annotator agreement for multi-label image annotation","year":2010,"categories":[101,98,99,67,176,63,107,90],"groups":[{"profile_id":1834051,"date_added":1294761039000,"group_id":782981},{"profile_id":2337001,"date_added":1285769336000,"group_id":422551},{"profile_id":11770,"date_added":1285613897000,"group_id":466061},{"profile_id":1962221,"date_added":1280219885000,"group_id":361951}],"file_hash":"3fe20026c3b16f9eab3ad8f460f5899b0c60c898","profile_ids":[1834051,2248521,4030661,1430431,2855921,11770,6383,2962871,1651881,6310,1962221,2337001,2079191],"oa_journal":false,"fullUrl":"\/research\/how-reliable-are-annotations-via-crowdsourcing-a-study-about-interannotator-agreement-for-multilabel-image-annotation\/","openurl":"ctx_ver=Z39.88-2004&rfr_id=info%3Asid%2Fmendeley.com%2Fmendeley&rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&rft.genre=article&rft.pages=557-566&rft.atitle=How+reliable+are+annotations+via+crowdsourcing%3A+a+study+about+inter-annotator+agreement+for+multi-label+image+annotation&rft.jtitle=Proceedings+of+the+international+conference+on+Multimedia+information+retrieval+MIR+10&rft.aulast=Nowak&rft.auinit1=S&rft.au=R%C3%BCger%2C+S&rft_id=info%3Adoi%2F10.1145%2F1743384.1743478"};
var currentUser = 0;

var docData = {
	docId: "",
	docTitle: "",
	profileId: "",
	groupId: "",
	documentKey: "",
	shareDoc: "",
	shareDocSource: "",
	makeFilesAvailable: "",
	canonical: "1",
	showingPdfPreview: "1"
};



$().ready(function(){
	// Alf's HACK reason why we have unbind followed by bind is because this template gets included twice 
	$(".added-to-library-button").click(function(event){ return false; });	
	$(".add-to-library-button").unbind("click", Mendeley.Catalog.addToLibraryHandler).bind("click", Mendeley.Catalog.addToLibraryHandler);
});
  
</script>            <a id="share-btn" href="" class="link-button btn-share">Share<span class="icon"></span></a>        </div>

</div>
</div><div class="column-a">
    <div class="side-nav">
    <ul>
        <li><a onclick="return false;" href="#" class="selected">Overview</a></li>
				            <li><a rel="nofollow" href="http://www.mendeley.com/research-papers/?rec=how-reliable-are-annotations-via-crowdsourcing-a-study-about-interannotator-agreement-for-multilabel-image-annotation">Related research</a></li>
            </ul>
    </div>
</div>
<div class="column-b">
    <div class="padding">

                <p>
                                    Proceedings of the international conference on Multimedia information retrieval MIR 10                 (2010)<br />
                                                <strong>Publisher:</strong> ACM Press, Pages: 557-566<br />
                    </p>
                                    <div id="identifiers-list">
                                <strong>ISBN:</strong> 9781605588155<br />                <strong>DOI:</strong> <a href="http://dx.doi.org/10.1145/1743384.1743478">10.1145/1743384.1743478</a><br />                                                </div>
                
    		
<div id="website-link">
<!-- TODO: data-log-click="article:sourcelink" -->
	
    <span data-log-click="article:sourcelink" data-dm-log-click="{&quot;event&quot;:&quot;click&quot;,&quot;page&quot;:&quot;articlePage&quot;,&quot;data[]&quot;:[&quot;sourceLinkClick&quot;,&quot;85f868f0-aed9-11df-a235-0024e8453de6&quot;,&quot;0&quot;,&quot;1&quot;]}">Available from <a rel='nofollow' class='' href='http://portal.acm.org/citation.cfm?doid=1743384.1743478'>portal.acm.org</a></span>
</div>
 
<script type="text/javascript">

var sourceLinkHandler = function(){
	
	
	var log = Mendeley.Util.decodeJson({"event":"click","page":"articlePage","data[]":["sourceLinkClick","85f868f0-aed9-11df-a235-0024e8453de6","0","1"]});
	Mendeley.Util.log(log);

};
// Alf's hack unbind followed by bind due to allow mulitple includes of this template
$(".identifiers-list a, #identifiers-list a").unbind("mousedown", sourceLinkHandler).bind("mousedown", sourceLinkHandler);

</script>

<style type="text/css">

#openurl-container { margin-top: 10px; }
.openurl-or { margin-right: 5px; }
#openurl-menu { width: 190px; padding: 5px 0; display: none; }

</style>

<span class="haz-drop-down" id="openurl-container">
	<span class="openurl-or">or </span><a class="drop-down link-button" id="openurl-menu-button" href="http://www.mendeley.com/account/import/?from-url=how-reliable-are-annotations-via-crowdsourcing-a-study-about-interannotator-agreement-for-multilabel-image-annotation#openurl-settings" title="Look up this article using an OpenURL resolver">Find this paper at:<span class="button-opener"></span></a>
	<div class="drop-down-pos">
		<div id="openurl-menu" class="drop-down-area">
			<ul>
								<li><a class="openurl-linkout" href="http://openurl.ac.uk?url_ver=Z39.88-2004&url_ctx_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Actx&ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fmendeley.com%2Fmendeley&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.pages=557-566&amp;rft.atitle=How+reliable+are+annotations+via+crowdsourcing%3A+a+study+about+inter-annotator+agreement+for+multi-label+image+annotation&amp;rft.jtitle=Proceedings+of+the+international+conference+on+Multimedia+information+retrieval+MIR+10&amp;rft.aulast=Nowak&amp;rft.auinit1=S&amp;rft.au=R%C3%BCger%2C+S&amp;rft_id=info%3Adoi%2F10.1145%2F1743384.1743478">openurl.ac.uk</a></li>
								<li><a class="openurl-linkout" href="http://worldcatlibraries.org/registry/gateway?url_ver=Z39.88-2004&url_ctx_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Actx&ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fmendeley.com%2Fmendeley&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.pages=557-566&amp;rft.atitle=How+reliable+are+annotations+via+crowdsourcing%3A+a+study+about+inter-annotator+agreement+for+multi-label+image+annotation&amp;rft.jtitle=Proceedings+of+the+international+conference+on+Multimedia+information+retrieval+MIR+10&amp;rft.aulast=Nowak&amp;rft.auinit1=S&amp;rft.au=R%C3%BCger%2C+S&amp;rft_id=info%3Adoi%2F10.1145%2F1743384.1743478">WorldCat®</a></li>
								<li><a class="openurl-linkout" href="http://scholar.google.co.uk/scholar?as_q=How+reliable+are+annotations+via+crowdsourcing%3A+a+study+about+inter-annotator+agreement+for+multi-label+image+annotation">Google Scholar</a></li>				<li class="divider">
					<a id="openurl-preferences-link" href="http://www.mendeley.com/account/import/?from-url=/research/how-reliable-are-annotations-via-crowdsourcing-a-study-about-interannotator-agreement-for-multilabel-image-annotation/#openurl-settings">Edit library access links</a>
				</li>
			</ul>
		</div>
	</div>
</span>

<script type="text/javascript">

$().ready(function(){
	$("#openurl-menu-button").click(function(e){
		$(this).toggleClass("open");
		$("#openurl-menu").toggle();
		return false;
	});
});

</script>
			<div id="abstract-container">
		<h4 class="underlined margin-top">Abstract</h4>
		<p>The creation of golden standard datasets is a costly business. Optimally more than one judgment per document is obtained to ensure a high quality on annotations. In this context, we explore how much annotations from experts differ from each other, how different sets of annotations influence the ranking of systems and if these annotations can be obtained with a crowdsourcing approach. This study is applied to annotations of images with multiple concepts. A subset of the images employed in the latest ImageCLEF Photo Annotation competition was manually annotated by expert annotators and non-experts with Mechanical Turk. The inter-annotator agreement is computed at an image-based and concept-based level using majority vote, accuracy and kappa statistics. Further, the Kendall and Kolmogorov-Smirnov correlation test is used to compare the ranking of systems regarding different ground-truths and different evaluation measures in a benchmark scenario. Results show that while the agreement between experts and non-experts varies depending on the measure used, its influence on the ranked lists of the systems is rather small. To sum up, the majority vote applied to generate one annotation set out of several opinions, is able to filter noisy judgments of non-experts to some extent. The resulting annotation set is of comparable quality to the annotations of experts.</p>
		</div>
	
	
    <div class='related-research' id="other-articles-container">
        <h4 class="underlined margin-top">Related research</h4>
                    <ol class="item-list documents">
		<li>
																	
																				
	
	<article class="item" id="document-af179080-6d07-11df-a2b2-0026b95e3eb7" style="" data-canonical-id="af179080-6d07-11df-a2b2-0026b95e3eb7" data-position="1">
		<div class="item-info">
			
			
							<h1>
										<a  href="/research/ef-ciently-learning-the-accuracy-of-labeling-sources-for-selective-sampling/" title="Efficiently learning the accuracy of labeling sources for selective sampling" 	 	
data-dm-log-click="{&quot;event&quot;:&quot;click&quot;,&quot;page&quot;:&quot;articlePageRelatedResearch&quot;,&quot;data[]&quot;:[&quot;Title&quot;,&quot;85f868f0-aed9-11df-a235-0024e8453de6&quot;,&quot;1&quot;,&quot;af179080-6d07-11df-a2b2-0026b95e3eb7&quot;,&quot;0&quot;]}" target="_parent">
													Efficiently learning the accuracy of labeling sources for selective sampling
											</a>
									</h1>
			
			
			<div class="authors">
																						Pinar Donmez, 																								Jaime G Carbonell, 																								Jeff Schneider															
									<span class="published_in">in <span class="publication">Proceedings of the 15th ACM International Conference on Knowledge Discovery and Data Mining (2009)</span>
				
									<span class="year">(2009)</span>
				
									</span>
								
			</div>

			
			<div class="actions">
                					<span class="reader-count" title="13 readers on Mendeley"><strong>13</strong> readers</span>
				
				
														<a rel="nofollow" class="add_button" id="addbtn_af179080-6d07-11df-a2b2-0026b95e3eb7" 	 	
data-dm-log-click="{&quot;event&quot;:&quot;click&quot;,&quot;page&quot;:&quot;articlePageRelatedResearch&quot;,&quot;data[]&quot;:[&quot;AddToLibrary&quot;,&quot;85f868f0-aed9-11df-a235-0024e8453de6&quot;,&quot;1&quot;,&quot;af179080-6d07-11df-a2b2-0026b95e3eb7&quot;,&quot;0&quot;]}"  data-join-overlay="document-add-to-library" href="#">
						<span id="addtext_af179080-6d07-11df-a2b2-0026b95e3eb7" class="addlib "><strong>Save reference</strong> to library</span>
					</a>
				
									<span class="slug">&#183;</span>										<a rel="nofollow" href='http://www.mendeley.com/research-papers/?rec=ef-ciently-learning-the-accuracy-of-labeling-sources-for-selective-sampling' rel='nofollow' 	 	
data-dm-log-click="{&quot;event&quot;:&quot;click&quot;,&quot;page&quot;:&quot;articlePageRelatedResearch&quot;,&quot;data[]&quot;:[&quot;moreLikeThis&quot;,&quot;85f868f0-aed9-11df-a235-0024e8453de6&quot;,&quot;1&quot;,&quot;af179080-6d07-11df-a2b2-0026b95e3eb7&quot;,&quot;0&quot;]}">
						<span><strong>Related</strong> research</span>
					</a>
				
				
				
				
				
							</div>
		</div>
	</article>
	</li>
	</ol>


<script type="text/javascript">
	
	
	var log = Mendeley.Util.decodeJson({"event":"PageLoad","page":"articlePageRelatedResearch","data[]":["Success","85f868f0-aed9-11df-a235-0024e8453de6"]});
	Mendeley.Util.log(log);

</script>            </div>

			<div id="citation-container">
<h4 class="underlined margin-top">Cite this document <sup>(BETA)</sup></h4>
<div class="tabbed-box">


<div class="tabs-general" id="">
	<ul>
					<li class="">
				<a href="#">APA</a>
			</li>
					<li class="">
				<a href="#">BibTeX</a>
			</li>
					<li class="">
				<a href="#">Cell</a>
			</li>
					<li class="">
				<a href="#">Chicago</a>
			</li>
					<li class="">
				<a href="#">Harvard</a>
			</li>
					<li class="">
				<a href="#">MLA</a>
			</li>
					<li class="">
				<a href="#">Nature</a>
			</li>
					<li class="">
				<a href="#">Science</a>
			</li>
			</ul>
</div>

	<script type="text/javascript">
	
		$(function(){
			$("div.tabs-general ul li a").click(function(e){
				// prevent default behaviour
				e.preventDefault();
				
				// deselect current selected tab
				$(this).parent().siblings(".selected").each(function(){
					$(this).removeClass("selected");
				});
				// select new tab
				$(this).parent().addClass("selected");
			});
		});
	
	</script>
<div style="overflow-x:auto;" class="tabbed-content">
      <p>APA</p>
</div>
<div style="overflow-x:auto;" class="tabbed-content">
      <p>BibTeX</p>
</div>
<div style="overflow-x:auto;" class="tabbed-content">
      <p>Cell</p>
</div>
<div style="overflow-x:auto;" class="tabbed-content">
      <p>Chicago</p>
</div>
<div style="overflow-x:auto;" class="tabbed-content">
      <p>Harvard</p>
</div>
<div style="overflow-x:auto;" class="tabbed-content">
      <p>MLA</p>
</div>
<div style="overflow-x:auto;" class="tabbed-content">
      <p>Nature</p>
</div>
<div style="overflow-x:auto;" class="tabbed-content">
      <p>Science</p>
</div>
<script type="text/javascript">
citation_data = transformJSON({"tags":["*****","qm: accuracy","qm: inter-annotator agreement","qm: kappa statistics","qm: kendall","qm: majority vote","task: image annotation","task: labelling"],"abstract":"The creation of golden standard datasets is a costly business. Optimally more than one judgment per document is obtained to ensure a high quality on annotations. In this context, we explore how much annotations from experts differ from each other, how different sets of annotations influence the ranking of systems and if these annotations can be obtained with a crowdsourcing approach. This study is applied to annotations of images with multiple concepts. A subset of the images employed in the latest ImageCLEF Photo Annotation competition was manually annotated by expert annotators and non-experts with Mechanical Turk. The inter-annotator agreement is computed at an image-based and concept-based level using majority vote, accuracy and kappa statistics. Further, the Kendall and Kolmogorov-Smirnov correlation test is used to compare the ranking of systems regarding different ground-truths and different evaluation measures in a benchmark scenario. Results show that while the agreement between experts and non-experts varies depending on the measure used, its influence on the ranked lists of the systems is rather small. To sum up, the majority vote applied to generate one annotation set out of several opinions, is able to filter noisy judgments of non-experts to some extent. The resulting annotation set is of comparable quality to the annotations of experts.","series":"MIR '10","identifiers":{"isbn":"9781605588155","doi":"10.1145\/1743384.1743478"},"website":"http:\/\/portal.acm.org\/citation.cfm?doid=1743384.1743478","stats":{"academic_status":{"3":2,"7":2,"5":4,"4":1,"11":4},"readers":13,"discipline":{"98":3,"67":1,"107":1,"90":2,"176":1,"63":1,"99":2,"101":2},"country":{"France":2,"United States":1,"Spain":3,"Denmark":1,"Germany":4,"United Kingdom":2}},"pages":"557-566","tags_counts":[3,3,3,3,3,3,3,3],"published_in":"Proceedings of the international conference on Multimedia information retrieval MIR 10","type":"journal","url":"how-reliable-are-annotations-via-crowdsourcing-a-study-about-interannotator-agreement-for-multilabel-image-annotation","publisher":"ACM Press","id":"85f868f0-aed9-11df-a235-0024e8453de6","fingerprint":620404966638274944,"authors":[{"forename":"S","surname":"Nowak"},{"forename":"S","surname":"R\u00fcger"}],"title":"How reliable are annotations via crowdsourcing: a study about inter-annotator agreement for multi-label image annotation","year":2010,"categories":[101,98,99,67,176,63,107,90],"groups":[{"profile_id":1834051,"date_added":1294761039000,"group_id":782981},{"profile_id":2337001,"date_added":1285769336000,"group_id":422551},{"profile_id":11770,"date_added":1285613897000,"group_id":466061},{"profile_id":1962221,"date_added":1280219885000,"group_id":361951}],"file_hash":"3fe20026c3b16f9eab3ad8f460f5899b0c60c898","profile_ids":[1834051,2248521,4030661,1430431,2855921,11770,6383,2962871,1651881,6310,1962221,2337001,2079191],"oa_journal":false,"fullUrl":"\/research\/how-reliable-are-annotations-via-crowdsourcing-a-study-about-interannotator-agreement-for-multilabel-image-annotation\/"});

var currentTab = 0;
$("div.tabs-general ul li a").click(function(clickedTab) {
        var thisTab = $(".tabbed-box .tabs-general ul li a").index(clickedTab.target);
        $(".tabbed-box .tabs-general li a").removeClass("active");
        $(".tabbed-box .tabs-general li a:eq("+thisTab+")").addClass("active");
        $(".tabbed-box .tabbed-content").hide();
        $(".tabbed-box .tabbed-content:eq("+thisTab+")").show();
        currentTab = thisTab;
});
$(window).load(function(){
	$("div.tabs-general ul li a:eq(0)").click();
	$(".tabbed-box .tabs-general li a").each(function(i){
		var name = $(this).text();
		var citeproc = new CSL.Engine(citeproc_sys,citeproc_styles[name]);
		citeproc.updateItems(["ITEM-1"]);
		var mybib = citeproc.makeBibliography();

		var tab = $(".tabbed-box .tabbed-content")[i];
		if (mybib && mybib.length && mybib[1].length)
			$(tab).html(mybib[1].join(""));
	});
});

</script>
</div>
</div>	    
    </div>
</div>

<div class="column-c">
	<div class="padding">
		<div id="share-this-page-container" style="display:none;"><div class="share-this-page2">
	<div class="url-wrapper">
		<input id="share-this-url" class="url-input" value="http://www.mendeley.com"/>
		<input id="share-this-urltype" class="checkbox" type="checkbox"/><label id="share-this-urltype-label" class="checkbox-label" for="share-this-urltype">Short URL</label>
	</div>
	<ul class="group-wrapper">
		<li><a name="fb_share" id="share-on-fb" class="share-button share-fb" rel="nofollow" href="#" target="_blank">Share on Facebook</a></li>
		<li><a id="share-on-twitter" class="share-button share-twitter" rel="nofollow" href="#" target="_blank">Share on Twitter</a></li>
		<li><a id="share-by-email" class="share-button share-email" rel="nofollow" href="#" target="_blank" >Email this link</a></li>
	</ul>
</div></div>
	    		<div id="pdf-thumbnail" style="height:346px;">
			<a href="#" id="pdf-underneath"><img src="http://s3.amazonaws.com/mendeley-pdf-previews/3f/e2/3fe20026c3b16f9eab3ad8f460f5899b0c60c898_tn.png" alt="" style="height:346px;" /></a>
			<a href="#" id="pdf-hover" style="height:346px;" 																																																																																				
	onmousedown='Mendeley.Util.log({
		"event": "click",
		"page": "articlePage",
		"data[]": ["pdfPreviewLightbox","85f868f0-aed9-11df-a235-0024e8453de6","0","1"]
	});'
><span>Click to zoom in</span> <em>2 pages available to preview</em></a>
		</div>
		<div id="reader-overlay">
    <div class="reader-position">
        <div id="reader">
            		
<div id="availability">
<!-- TODO: data-log-click="article:sourcelink" -->
	
    <span data-log-click="article:sourcelink" data-dm-log-click="{&quot;event&quot;:&quot;click&quot;,&quot;page&quot;:&quot;articlePage&quot;,&quot;data[]&quot;:[&quot;sourceLinkClick&quot;,&quot;85f868f0-aed9-11df-a235-0024e8453de6&quot;,&quot;0&quot;,&quot;1&quot;]}">Available from <a rel='nofollow' class='' href='http://portal.acm.org/citation.cfm?doid=1743384.1743478'>portal.acm.org</a></span>
</div>
 
<script type="text/javascript">

var sourceLinkHandler = function(){
	
	
	var log = Mendeley.Util.decodeJson({"event":"click","page":"articlePage","data[]":["sourceLinkClick","85f868f0-aed9-11df-a235-0024e8453de6","0","1"]});
	Mendeley.Util.log(log);

};
// Alf's hack unbind followed by bind due to allow mulitple includes of this template
$(".identifiers-list a, #identifiers-list a").unbind("mousedown", sourceLinkHandler).bind("mousedown", sourceLinkHandler);

</script>

            <div id="reader-buttons">
                                    <a href="" id="pdf-close" class="link-button btn-close right"><span class="icon"></span>Close</a>
                                                    
						
									

<style type="text/css">

	.add-to-library-buttons .btn-add { display: inline-block; }
	.add-to-library-buttons .btn-added { display: none; }
	.add-to-library-buttons.inLibrary .btn-add { display: none; }
	.add-to-library-buttons.inLibrary .btn-added { display: inline-block; }

</style>

<span class="add-to-library-buttons ">
	<a href="#added-to-library" class="added-to-library-button link-button primary btn-added"><span class="icon"></span>In your library</a>
	<a href="#add-to-library" class="add-to-library-button link-button primary btn-add" data-log-click="article:add-to-library"
		data-dm-log-click="{&quot;event&quot;:&quot;click&quot;,&quot;page&quot;:&quot;articlePage&quot;,&quot;data[]&quot;:[&quot;AddToLibrary&quot;,&quot;85f868f0-aed9-11df-a235-0024e8453de6&quot;,&quot;0&quot;,&quot;1&quot;]}"><span class="icon icon-save-without-files"></span>Save reference to library</a>
</span>

<script type="text/javascript">
var metadata = {"tags":["*****","qm: accuracy","qm: inter-annotator agreement","qm: kappa statistics","qm: kendall","qm: majority vote","task: image annotation","task: labelling"],"abstract":"The creation of golden standard datasets is a costly business. Optimally more than one judgment per document is obtained to ensure a high quality on annotations. In this context, we explore how much annotations from experts differ from each other, how different sets of annotations influence the ranking of systems and if these annotations can be obtained with a crowdsourcing approach. This study is applied to annotations of images with multiple concepts. A subset of the images employed in the latest ImageCLEF Photo Annotation competition was manually annotated by expert annotators and non-experts with Mechanical Turk. The inter-annotator agreement is computed at an image-based and concept-based level using majority vote, accuracy and kappa statistics. Further, the Kendall and Kolmogorov-Smirnov correlation test is used to compare the ranking of systems regarding different ground-truths and different evaluation measures in a benchmark scenario. Results show that while the agreement between experts and non-experts varies depending on the measure used, its influence on the ranked lists of the systems is rather small. To sum up, the majority vote applied to generate one annotation set out of several opinions, is able to filter noisy judgments of non-experts to some extent. The resulting annotation set is of comparable quality to the annotations of experts.","series":"MIR '10","identifiers":{"isbn":"9781605588155","doi":"10.1145\/1743384.1743478"},"website":"http:\/\/portal.acm.org\/citation.cfm?doid=1743384.1743478","stats":{"academic_status":{"3":2,"7":2,"5":4,"4":1,"11":4},"readers":13,"discipline":{"98":3,"67":1,"107":1,"90":2,"176":1,"63":1,"99":2,"101":2},"country":{"France":2,"United States":1,"Spain":3,"Denmark":1,"Germany":4,"United Kingdom":2}},"pages":"557-566","tags_counts":[3,3,3,3,3,3,3,3],"published_in":"Proceedings of the international conference on Multimedia information retrieval MIR 10","type":"journal","url":"how-reliable-are-annotations-via-crowdsourcing-a-study-about-interannotator-agreement-for-multilabel-image-annotation","publisher":"ACM Press","id":"85f868f0-aed9-11df-a235-0024e8453de6","fingerprint":620404966638274944,"authors":[{"forename":"S","surname":"Nowak"},{"forename":"S","surname":"R\u00fcger"}],"title":"How reliable are annotations via crowdsourcing: a study about inter-annotator agreement for multi-label image annotation","year":2010,"categories":[101,98,99,67,176,63,107,90],"groups":[{"profile_id":1834051,"date_added":1294761039000,"group_id":782981},{"profile_id":2337001,"date_added":1285769336000,"group_id":422551},{"profile_id":11770,"date_added":1285613897000,"group_id":466061},{"profile_id":1962221,"date_added":1280219885000,"group_id":361951}],"file_hash":"3fe20026c3b16f9eab3ad8f460f5899b0c60c898","profile_ids":[1834051,2248521,4030661,1430431,2855921,11770,6383,2962871,1651881,6310,1962221,2337001,2079191],"oa_journal":false,"fullUrl":"\/research\/how-reliable-are-annotations-via-crowdsourcing-a-study-about-interannotator-agreement-for-multilabel-image-annotation\/","openurl":"ctx_ver=Z39.88-2004&rfr_id=info%3Asid%2Fmendeley.com%2Fmendeley&rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&rft.genre=article&rft.pages=557-566&rft.atitle=How+reliable+are+annotations+via+crowdsourcing%3A+a+study+about+inter-annotator+agreement+for+multi-label+image+annotation&rft.jtitle=Proceedings+of+the+international+conference+on+Multimedia+information+retrieval+MIR+10&rft.aulast=Nowak&rft.auinit1=S&rft.au=R%C3%BCger%2C+S&rft_id=info%3Adoi%2F10.1145%2F1743384.1743478"};
var currentUser = 0;

var docData = {
	docId: "",
	docTitle: "",
	profileId: "",
	groupId: "",
	documentKey: "",
	shareDoc: "",
	shareDocSource: "",
	makeFilesAvailable: "",
	canonical: "1",
	showingPdfPreview: "1"
};



$().ready(function(){
	// Alf's HACK reason why we have unbind followed by bind is because this template gets included twice 
	$(".added-to-library-button").click(function(event){ return false; });	
	$(".add-to-library-button").unbind("click", Mendeley.Catalog.addToLibraryHandler).bind("click", Mendeley.Catalog.addToLibraryHandler);
});
  
</script>                <a href="" id="toggletext" class="link-button btn-plaintext" title="Toggle plain text" rel="tipsy"><span class="icon loading"></span></a>
            </div>


            <div id="viewer" class="show-pdf">
             <div id="scroller">                                    <div class="page-number" id="page-1">Page 1</div>
                    <div class="pdf-page" id="page-img-container-1">
                                                <img id="pdf-page-image-1" src="http://www.mendeley.com/graphics/common/spacer_4223231388365693.gif" alt="hidden" class="pdf-image"  height="955" style="height:955px;" />
                        <h1> How reliable are annotations via crowdsourcing: a study about inter-annotator agreement for multi-label image annotation</h1>                        <div class="plaintext">How Reliable are Annotations via Crowdsourcing?<br />
A Study about Inter-annotator Agreement for Multi-label Image Annotation<br />
Stefanie Nowak<br />
Fraunhofer IDMT<br />
Ehrenbergstr. 31<br />
98693 Ilmenau, Germany<br />
stefanie.nowak@idmt.fraunhofer.de<br />
Stefan Rüger<br />
Knowledge Media Institute<br />
The Open University<br />
Walton Hall, Milton Keynes, MK7 6AA, UK<br />
s.rueger@open.ac.uk<br />
ABSTRACT<br />
The creation of golden standard datasets is a costly business.<br />
Optimally more than one judgment per document is ob-<br />
tained to ensure a high quality on annotations. In this con-<br />
text, we explore how much annotations from experts differ<br />
from each other, how different sets of annotations influence<br />
the ranking of systems and if these annotations can be ob-<br />
tained with a crowdsourcing approach. This study is applied<br />
to annotations of images with multiple concepts. A sub-<br />
set of the images employed in the latest ImageCLEF Photo<br />
Annotation competition was manually annotated by expert<br />
annotators and non-experts with Mechanical Turk. The<br />
inter-annotator agreement is computed at an image-based<br />
and concept-based level using majority vote, accuracy and<br />
kappa statistics. Further, the Kendall τ and Kolmogorov-<br />
Smirnov correlation test is used to compare the ranking of<br />
systems regarding different ground-truths and different eval-<br />
uation measures in a benchmark scenario. Results show that<br />
while the agreement between experts and non-experts varies<br />
depending on the measure used, its influence on the ranked<br />
lists of the systems is rather small. To sum up, the majority<br />
vote applied to generate one annotation set out of several<br />
opinions, is able to filter noisy judgments of non-experts to<br />
some extent. The resulting annotation set is of comparable<br />
quality to the annotations of experts.<br />
Categories and Subject Descriptors<br />
D.2.8 [Software Engineering]: Metrics—complexity mea-<br />
sures, performance measures ; H.3.4 [Information Storage<br />
and Retrieval]: Systems and Software—Performance eval-<br />
uation (efficiency and effectiveness)<br />
General Terms<br />
Experimentation, Human Factors, Measurement, Performance<br />
Keywords<br />
Inter-annotator Agreement, Crowdsourcing<br />
Permission to make digital or hard copies of all or part of this work for<br />
personal or classroom use is granted without fee provided that copies are<br />
not made or distributed for profit or commercial advantage and that copies<br />
bear this notice and the full citation on the first page. To copy otherwise, to<br />
republish, to post on servers or to redistribute to lists, requires prior specific<br />
permission and/or a fee.<br />
MIR’10, March 29–31, 2010, Philadelphia, Pennsylvania, USA.<br />
Copyright 2010 ACM 978-1-60558-815-5/10/03 ...$10.00.<br />
1. INTRODUCTION<br />
In information retrieval and machine learning, golden stan-<br />
dard databases play a crucial role. They allow to compare<br />
the effectiveness and quality of systems. Depending on the<br />
application area, creating large, semantically annotated cor-<br />
pora from scratch is a time and cost consuming activity.<br />
Usually experts review the data and perform manual an-<br />
notations. Often different annotators judge the same data<br />
and the inter-annotator agreement is computed among their<br />
judgments to ensure quality. Ambiguity of data and task<br />
have a direct effect on the agreement factor.<br />
The goal of this work is twofold. First, we investigate<br />
how much several sets of expert annotations differ from each<br />
other in order to see whether repeated annotation is neces-<br />
sary and if it influences performance ranking in a bench-<br />
mark scenario. Second, we explore if non-expert annota-<br />
tions are reliable enough to provide ground-truth annota-<br />
tions for a benchmarking campaign. Therefore, four exper-<br />
iments on inter-annotator agreement are conducted applied<br />
to the annotation of an image corpus with multiple labels.<br />
The dataset used is a subset of the MIR Flickr 25,000 image<br />
dataset [12]. 18,000 Flickr photos of this dataset annotated<br />
with 53 concepts were utilized in the latest ImageCLEF 2009<br />
Photo Annotation Task [19] in which 19 research teams sub-<br />
mitted 74 run configurations. Due to time and cost restric-<br />
tions most images of this task were annotated by only one<br />
expert annotator. We conduct the experiments on a small<br />
subset of 99 images. For our experiments, 11 different ex-<br />
perts annotated the complete set, so that each image was<br />
annotated 11 times. Further, the set was distributed over<br />
Amazon Mechanical Turk (MTurk) to non-expert annota-<br />
tors all over the world, who labelled it nine times. The<br />
inter-annotator agreement as well as the system ranking for<br />
the 74 submissions is calculated by considering each anno-<br />
tation set as single ground-truth.<br />
The remainder of the paper is organized as follows. Sec. 2<br />
describes the related work on obtaining inter-annotator agree-<br />
ments and crowdsourcing approaches for distributed data<br />
annotation. Sec. 3 explains the setup of the experiments by<br />
illustrating the dataset and the annotation acquisition pro-<br />
cess. Sec. 4 details the methodology of the experiments and<br />
introduces the relevant background. Finally, Sec. 5 presents<br />
and discusses the results of the four experiments and we<br />
conclude in Sec. 6.<br />
2. RELATED WORK<br />
Over the years a fair amount of work on how to prepare<br />
golden standard databases for information retrieval eval-<br />
557<br />
</div>                    </div>
                                    <div class="page-number" id="page-2">Page 2</div>
                    <div class="pdf-page" id="page-img-container-2">
                                                <img id="pdf-page-image-2" src="http://www.mendeley.com/graphics/common/spacer_4223231388365693.gif" alt="hidden" class="pdf-image"  height="955" style="height:955px;" />
                                                <div class="plaintext">uation has been published. One important point in as-<br />
sessing ground-truth for databases is to consider the agree-<br />
ment among annotators. The inter-annotator agreement de-<br />
scribes the degree of consensus and homogeneity in judg-<br />
ments among annotators. Kilgarriff [15] proposes guidelines<br />
on how to produce a golden standard dataset for benchmark-<br />
ing campaigns for word-sense disambiguation. He concludes<br />
that the annotators and the vocabulary used during anno-<br />
tation assessment have to be chosen with care while the re-<br />
sources should be used effectively. Kilgarriff states that it re-<br />
quires more than one person to assign word senses, that one<br />
should calculate the inter-annotator agreement and deter-<br />
mine whether it is high enough. He identifies three reasons<br />
that can lead to ambiguous annotations and suggests ways<br />
how to solve them. Basically the reasons lie in the ambiguity<br />
of data, poor definition of annotation scheme or mistakes of<br />
annotators due to lack of motivation or knowledge.<br />
To assess the subjectivity in ground-truthing in multime-<br />
dia information retrieval evaluation, several work has been<br />
performed on the analysis of inter-annotator agreements.<br />
Voorhees [28] analyses the influence of changes in relevance<br />
judgments on the evaluation of retrieval results utilizing the<br />
Kendall τ correlation coefficient. Volkmer et al. [27] present<br />
an approach that integrates multiple judgments in the clas-<br />
sification system and compare them to the kappa statistics.<br />
Brants proposes in [2] a study about inter-annotator agree-<br />
ment for part-of-speech and structural information annota-<br />
tion in a corpus of German newspapers. He uses the accu-<br />
racy and F-score between the annotated corpus of two an-<br />
notators to assess their agreement. A few studies have been<br />
performed to study the inter-annotator agreement for word<br />
sense disambiguation [26, 5]. These studies often utilize<br />
kappa statistics for calculating agreement between judges.<br />
Recently, different works were presented that outsource<br />
multimedia annotation tasks to crowdsourcing approaches.<br />
According to Howe [10],<br />
crowdsourcing represents the act of a company<br />
or institution taking a function once performed<br />
by employees and outsourcing it to an undefined<br />
(and generally large) network of people in the<br />
form of an open call.<br />
Often the work is distributed over web-based platforms. Uti-<br />
lizing crowdsourcing approaches for assessing ground-truth<br />
corpora is mainly motivated by the reduction of costs and<br />
time. The annotation task is divided into small parts and<br />
distributed to a large community. Sorokin et al. [25] were<br />
one of the first who outsourced image segmentation and la-<br />
belling tasks to MTurk. The ImageNet database [7] was<br />
constructed by utilizing workers at MTurk that validated if<br />
images depict the concept of a certain WordNet node.<br />
Some studies have been conducted that explore the an-<br />
notation qualities obtained with crowdsourcing approaches.<br />
Alonso and Mizarro [1] examine how well relevance judg-<br />
ments for the TREC topic about space program can be ful-<br />
filled by workers at MTurk. The relevance of a document<br />
had to be judged regarding this topic and the authors com-<br />
pared the results of the non-experts to the relevance assess-<br />
ment of TREC. They found that the annotations among<br />
non-expert and TREC assessors are of comparable quality.<br />
Hsueh et al. [11] compare the annotation quality of senti-<br />
ment in political blog snippets from a crowdsourcing ap-<br />
proach and expert annotators. They define three criteria,<br />
the noise level, the sentiment ambiguity, and the lexical un-<br />
certainty, that can be used to identify high quality annota-<br />
tions. Snow et al. [24] investigate the annotation quality for<br />
non-expert annotators in five natural language tasks. They<br />
found that a small number of non-expert annotations per<br />
item yields to equal performance to an expert annotator<br />
and propose to model the bias and reliability of individual<br />
workers for an automatic noise correction algorithm. Kazai<br />
and Milic-Frayling [13] examine measures to obtain the qual-<br />
ity of collected relevance assessments. They point to several<br />
issues like topic and content familiarity, dwell time, agree-<br />
ment or comments of workers that can be used to derive a<br />
trust weight for judgments. Other work deals with how to<br />
verify crowdsourced annotations [4], how to deal with sev-<br />
eral noisy labellers [23, 8] and how to balance pricing for<br />
crowdsourcing [9].<br />
Following the work of [25, 7], we obtained annotations for<br />
images utilizing MTurk. In our experiments, these annota-<br />
tions are acquired on an image-based level for a multi-label<br />
scenario and compared to expert annotations. Extending<br />
the work that was performed on inter-annotator agreement<br />
[1, 2], we do not just analyse the inter-rater agreement, but<br />
study the effect of multiple annotation sets on the ranking<br />
of systems in a benchmark scenario.<br />
3. EXPERIMENTAL SETUP<br />
In this section, we describe the setup of our experiments.<br />
First, the dataset used for the experiments on annotator<br />
agreements is briefly explained. Next, the process of ob-<br />
taining expert annotations is illustrated by outlining the de-<br />
sign of our annotation tool and the task the experts had<br />
to perform. Following, the acquisition process of obtaining<br />
ground-truth from MTurk is detailed. Finally, the workflow<br />
of posing tasks at Amazon MTurk, designing the annotation<br />
template, obtaining and filtering the results is highlighted.<br />
3.1 Dataset<br />
The experiments are conducted on a subset of 99 images<br />
from the MIR Flickr Image Dataset [12]. The MIR Flickr<br />
Image Dataset consists of 25,000 Flickr images. It was uti-<br />
lized for a multi-label image annotation task at the latest Im-<br />
ageCLEF 2009 [19] competition. Altogether, 18,000 of the<br />
images were annotated with 53 visual concepts by expert<br />
annotators of the Fraunhofer IDMT research staff. 5,000<br />
images with annotations were provided as training set and<br />
the performance of the annotation systems was evaluated on<br />
13,000 images. 19 research teams submitted a total of 74 run<br />
configurations. The 99 images utilized in our experiments<br />
on inter-annotator agreements and its influence on system<br />
ranking are part of the testset of the Photo Annotation Task.<br />
Consequently, the results of 74 system configurations in au-<br />
tomated annotation of these images can serve as basis for<br />
investigating the influence on ranking.<br />
3.2 Collecting Data of Expert Annotators<br />
The set of 99 images was annotated by 11 expert annota-<br />
tors from the Fraunhofer IDMT research staff with 53 con-<br />
cepts. We provided the expert annotators a definition of<br />
each concept including example photos (see [18] for a de-<br />
tailed description of the concepts.). The 53 concepts to be<br />
annotated per image were ordered into several categories. In<br />
principle, there were two different kinds of concepts, optional<br />
concepts and mutual exclusive concepts. E.g. the category<br />
558<br />
</div>                    </div>
                </div>            </div>

            <div class="top-shadow"></div>
            <div class="bottom-shadow"></div>
        </div>

    </div></div>

<script type="text/javascript">
	Mendeley.PDFReader.addPageImage('1', 'http://s3.amazonaws.com/mendeley-pdf-previews/3f/e2/3fe20026c3b16f9eab3ad8f460f5899b0c60c898_1.png');
	Mendeley.PDFReader.addPageImage('2', 'http://s3.amazonaws.com/mendeley-pdf-previews/3f/e2/3fe20026c3b16f9eab3ad8f460f5899b0c60c898_2.png');

jQuery(document).ready(function() {
    Mendeley.PDFReader.initialise(1);
});
</script>				
		<div id="article-signup-box">
	<div id="article-signup-box-main">
		<div id="article-signup-box-heading">Mendeley is Free. Sign up</div>
		<div id="article-signup-box-subheading">and discover more papers in <a rel="nofollow" href="http://www.mendeley.com/research-papers/computer-and-information-science/">Computer and Information Science</a>.</div>		
		<div id="article-signup-box-description-container">
			<div id="article-signup-box-description">Mendeley is a paper search and productivity tool. Sign up today, find more relevant papers and organise your research all in one place.</div>
		</div>
	</div>
	
	<form method="post" action="https://www.mendeley.com/join/create/" data-log-submit="article-signup-box-submit"><input type="hidden" name="csrf_token" value="6d9061a74123e525b2ac4178bc9c281d6e9ac338">
		<div class="form-item">
			<label for="article-signup-box-fname">First name</label>
			<input type="text" name="fname" id="article-signup-box-fname">
		</div>
	
		<div class="form-item">
			<label for="article-signup-box-lname">Last name</label>
			<input type="text" name="lname" id="article-signup-box-lname">
		</div>
	
		<div class="form-item">
			<label for="article-signup-box-email">E-mail address</label>
			<input type="text" name="email" id="article-signup-box-email">
		</div>
		
		<input type="hidden" name="trackingUrl" value="/join/lazyreg/articles/pdf/">
		
		<div id="article-signup-box-submit-container"><input type="submit" id="article-signup-box-submit" class="link-button primary" value="Sign Up"></div>
	</form>
	
	<div id="article-signup-box-footer">
		<span>...or sign in with</span>
		<a href="#" onclick="Mendeley.FacebookConnect.login('');return false;" class="fb-auth" data-log-click="article:signup-box/facebook-signin">Facebook</a>
	</div>
</div>


<style type="text/css">

#article-signup-box { -moz-border-radius: 10px; -webkit-border-radius: 10px; border-radius: 10px; margin-bottom: 20px; border: 1px solid #D2D2D2; background: url("/graphics/commonnew/laptop_small.jpg") no-repeat scroll 137px 40px #fff;  }
#article-signup-box-main { padding: 5px 10px 10px; }
#article-signup-box-heading { font-size: 18px; font-family: "ff-meta-serif-web-pro-1","ff-meta-serif-web-pro-2",Georgia,serif; }
#article-signup-box-description-container { height: 120px; margin: 30px 0 10px 0;}
#article-signup-box-description { width: 140px; font-size: 11px; line-height: 15px; }
#article-signup-box form { margin-bottom: 10px; }
#article-signup-box .form-item label { width: 90px; display: inline-block; text-align: right; padding: 2px 5px; }
#article-signup-box .form-item input { width: 150px; display: inline-block; padding: 5px 2px; }
#article-signup-box-footer { border-top: 1px solid #aaa; background-color: #eee; padding: 10px; -moz-border-radius: 0 0 10px 10px; -webkit-border-radius: 0 0 10px 10px; border-radius: 0 0 10px 10px;}
#article-signup-box-submit-container { padding-left: 105px; }
#article-signup-box-submit { width: 157px; }
#article-signup-box .fb-auth { margin-left: 10px; }

</style>
		
        <h3 class="underlined">Readership Statistics</h3>

	<div class="reader-stats">
		<div id="total-reader-count"><b>13</b> Readers on Mendeley</div>
		
				<div class="text-minor reader-count-by">by Discipline</div>
					<div class="reader-stats-bar">
				<div style="width:100%;height:100%;background-color:#c3c3c3;">&nbsp;</div>
			</div>
			<div class="reader-stats-figure">77% Computer and Information Science</div>
					<div class="reader-stats-bar">
				<div style="width:19%;height:100%;background-color:#c3c3c3;">&nbsp;</div>
			</div>
			<div class="reader-stats-figure">15% Business Administration</div>
					<div class="reader-stats-bar">
				<div style="width:10%;height:100%;background-color:#c3c3c3;">&nbsp;</div>
			</div>
			<div class="reader-stats-figure">8% Electrical and Electronic Engineering</div>
						
				<div class="text-minor reader-count-by">by Academic Status</div>
					<div class="reader-stats-bar">
				<div style="width:100%;height:100%;background-color:#c3c3c3;">&nbsp;</div>
			</div>
			<div class="reader-stats-figure">31% Student (Master)</div>
					<div class="reader-stats-bar">
				<div style="width:100%;height:100%;background-color:#c3c3c3;">&nbsp;</div>
			</div>
			<div class="reader-stats-figure">31% Ph.D. Student</div>
					<div class="reader-stats-bar">
				<div style="width:48%;height:100%;background-color:#c3c3c3;">&nbsp;</div>
			</div>
			<div class="reader-stats-figure">15% Assistant Professor</div>
				
			</div>
	
		<div class="reader-stats-footer">
		<span class="reader-stats-footer-text">Want more statistics?</span>
		<a class="link-button" id="stats-widget-sign-up" data-log-click="article:stats-widget/sign-up" data-join-overlay="stats-widget/pdf">Sign up</a>
	</div>
	        
        
                    <div id="tags-container">
            <h4 class="underlined margin-top">Tags</h4>
            <div class="tags-list">
                <a href="http://www.mendeley.com/research-papers/tag/%2A%2A%2A%2A%2A/" class="tag" rel="tag">*****<span></span></a><a href="http://www.mendeley.com/research-papers/tag/qm%3A+accuracy/" class="tag" rel="tag">qm: accuracy<span></span></a><a href="http://www.mendeley.com/research-papers/tag/qm%3A+inter-annotator+agreement/" class="tag" rel="tag">qm: inter-annotator agreement<span></span></a><a href="http://www.mendeley.com/research-papers/tag/qm%3A+kappa+statistics/" class="tag" rel="tag">qm: kappa statistics<span></span></a><a href="http://www.mendeley.com/research-papers/tag/qm%3A+kendall/" class="tag" rel="tag">qm: kendall<span></span></a><a href="http://www.mendeley.com/research-papers/tag/qm%3A+majority+vote/" class="tag" rel="tag">qm: majority vote<span></span></a><a href="http://www.mendeley.com/research-papers/tag/task%3A+image+annotation/" class="tag" rel="tag">task: image annotation<span></span></a><a href="http://www.mendeley.com/research-papers/tag/task%3A+labelling/" class="tag" rel="tag">task: labelling<span></span></a>            </div>
            </div>
        	</div>
</div>

<script type="text/javascript">
									
	
	var log = Mendeley.Util.decodeJson({"event":"PageLoad","page":"articlePage","data[]":["Success","85f868f0-aed9-11df-a235-0024e8453de6","0","1"]});
	Mendeley.Util.log(log);

</script>


<script type="text/javascript">
$('body').click(function(event) {
    if (!$(event.target).closest('.discipline_widget').length && !$(event.target).closest('.tab').length && $('.discipline_widget:hidden').length == 0) {
        $('.discipline_widget').hide();
    };
});
</script>


						<div class="clear"></div>
					</div>
				</div>
			</div>
			
			<div id="footer">
				<div class="footer-content">
					<ul id="footer-links">
						<li>
							<dl>
								<dt>How Mendeley Works</dt>
																<dd><a href="http://www.mendeley.com/bibliography-maker-database-generator/" title="Bibliography maker">Bibliography database</a></dd>
								<dd><a href="http://www.mendeley.com/manage-annotate-pdf-research-papers/">Manage papers and PDFs</a></dd>
								<dd><a href="http://www.mendeley.com/citation-generator-maker/" title="Citation generator">Citation Generator</a></dd>
								<dd><a href="http://www.mendeley.com/document-sharing-research-collaboration/">Research Collaboration</a></dd>
								<dd><a href="http://www.mendeley.com/research-paper-recommendations-research-trends/" title="Research trends">Research trends</a></dd>
								<dd><a href="http://dev.mendeley.com/" title="Mendeley Open API">Open API</a></dd>
							</dl>
							<dl>
								<dt>About Mendeley</dt>
								<dd><a href="http://www.mendeley.com/blog/" rel="nofollow">Blog</a></dd>
								<dd><a href="http://www.mendeley.com/blog/jobs/" rel="nofollow">Jobs</a></dd>
								<dd><a href="http://www.mendeley.com/about-us/" rel="nofollow">About Us</a></dd>
								<dd><a href="http://www.mendeley.com/review/" rel="nofollow">Reviews</a></dd>
								<dd><a href="http://www.mendeley.com/awards-endorsements/" rel="nofollow">Awards &amp; Endorsements</a></dd>
								<dd><a href="http://www.mendeley.com/spread-the-word/" rel="nofollow">Spread the Word</a></dd>
								<dd><a href="http://www.mendeley.com/advisors/activities/" rel="nofollow">Mendeley Advisors</a></dd>
								<dd><a href="http://www.mendeley.com/advisors/" rel="nofollow">Become an Advisor</a></dd>
								<dd><a href="http://www.mendeley.com/contact-us/" rel="nofollow">Contact Us</a></dd>
							</dl>
							<dl>
								<dt>Download and Network</dt>
								<!-- id needed for general_loggin -->
								<dd><a id="footer-download-mendeley" href="http://www.mendeley.com/download-mendeley-desktop/" rel="nofollow" data-log-click="footer/download">Download Mendeley</a></dd>
								<dd><a href="http://www.mendeley.com/search/">Search User Profiles</a></dd>
								<dd><a href="http://www.mendeley.com/find/" rel="nofollow">Find Contacts</a></dd>
								<dd><a href="http://www.mendeley.com/invite/" rel="nofollow">Invite Contacts</a></dd>
								<dd><a href="http://www.mendeley.com/directory/">People Directory</a></dd>
								<dd><a href="http://www.mendeley.com/import/" rel="nofollow">Install Web Importer</a></dd>
							</dl>
							<dl>
								<dt>Get Support</dt>
								<dd><a href="http://www.mendeley.com/getting-started/" rel="nofollow">Getting Started</a></dd>
								<dd><a href="http://www.mendeley.com/faq/" rel="nofollow">FAQ</a></dd>
								<dd><a href="http://www.mendeley.com/contact-support/" rel="nofollow">Contact Support</a></dd>
								<dd><a href="http://feedback.mendeley.com" rel="nofollow">Feedback</a></dd>
								<dd><a href="http://www.mendeley.com/citationstyles/" title="APA, MLA citation styles">Citation Styles</a></dd>
								<dd><a href="http://www.mendeley.com/release-notes/" rel="nofollow">Release Notes</a></dd>
							</dl>
							<dl>
								<dt>Other</dt>
								<dd><a href="http://www.mendeley.com/upgrade/">Upgrade Mendeley</a></dd>
								<dd><a href="http://www.mendeley.com/terms/" rel="nofollow">Terms</a></dd>
								<dd><a href="http://www.mendeley.com/privacy/" rel="nofollow">Privacy</a></dd>
								<dd><a href="http://www.mendeley.com/copyright/" rel="nofollow">Copyright</a></dd>
							</dl>
						</li>
					</ul>
				</div>
				<div class="clear"></div>
			</div>
		</div>


<!--[if IE 6]>&nbsp;<![endif]-->
	<script type="text/javascript" src="http://www.mendeley.com/min.php/mendeley_1173445428378941"></script>
	
	
	
		<script type="text/javascript">
		_gaq.push(['_setCustomVar', 1, 'get-mendeley-button', "signup", 2]);
	_gaq.push(['_setCustomVar', 2, 'article-signup-banner', "false", 2]);
	_gaq.push(['_trackPageview']);

	</script>
	



	
	<script type="text/javascript">
		//<![CDATA[
		$(document).ready(function() {
			if(readCookie('__munv')) {
				$('#verification-reminder').show();
			}	
		});	
		function showLogin() {
			$('#header-login-form').show();
			$('#verification-reminder').hide();
			return false;
		}
		function readCookie(name) {
			var nameEQ = name + "=";
			var ca = document.cookie.split(';');
			for(var i=0;i < ca.length;i++) {
				var c = ca[i];
				while (c.charAt(0)==' ') c = c.substring(1,c.length);
				if (c.indexOf(nameEQ) == 0) return c.substring(nameEQ.length,c.length);
				}
			return null;
		}
		//]]>				
	</script>
		

<script type="text/javascript">

$().ready(Mendeley.Logging.init);

</script>
	
<div id="fb-root"></div>
<script>

  FB.init({appId: '10150110947375581', status: true, cookie: true,
             xfbml: true});

</script>

</body>
</html>