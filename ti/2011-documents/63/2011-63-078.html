<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" dir="ltr" lang="en-US">
<head profile="http://gmpg.org/xfn/11">
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8" />
<title>Participants</title>
<meta name="robots" content="noodp, noydir" />
<meta name="description" content="Below is a list of researchers who participated at the Symposium. Marcella Baker Affiliation: Carnegie Mellon University Cindy L. Bethel Affiliation: Yale University Email: cindy.bethel { at } yale.edu Web-page: http://www.cindybethel.com My research interests focus on how intelligent affective agents can recognize and model" />
<link rel="stylesheet" href="http://hci.cs.wisc.edu/aaai10/wp-content/themes/thesis_18/style.css?012511-183217" type="text/css" media="screen, projection" />
<link rel="stylesheet" href="http://hci.cs.wisc.edu/aaai10/wp-content/themes/thesis_18/custom/layout.css?012611-12220" type="text/css" media="screen, projection" />
<!--[if lte IE 8]><link rel="stylesheet" href="http://hci.cs.wisc.edu/aaai10/wp-content/themes/thesis_18/lib/css/ie.css?012511-183213" type="text/css" media="screen, projection" /><![endif]-->
<link rel="stylesheet" href="http://hci.cs.wisc.edu/aaai10/wp-content/themes/thesis_18/custom/custom.css?012611-12909" type="text/css" media="screen, projection" />
<link rel="canonical" href="http://hci.cs.wisc.edu/aaai10/?page_id=160" />
<link rel="alternate" type="application/rss+xml" title="Dialog with Robots RSS Feed" href="http://hci.cs.wisc.edu/aaai10/?feed=rss2" />
<link rel="pingback" href="http://hci.cs.wisc.edu/aaai10/xmlrpc.php" />
<link rel="EditURI" type="application/rsd+xml" title="RSD" href="http://hci.cs.wisc.edu/aaai10/xmlrpc.php?rsd" />
<link rel='stylesheet' id='NextGEN-css'  href='http://hci.cs.wisc.edu/aaai10/wp-content/plugins/nextgen-gallery/css/nggallery.css?ver=1.0.0' type='text/css' media='screen' />
<link rel='stylesheet' id='shutter-css'  href='http://hci.cs.wisc.edu/aaai10/wp-content/plugins/nextgen-gallery/shutter/shutter-reloaded.css?ver=1.3.0' type='text/css' media='screen' />
<script type='text/javascript'>
/* <![CDATA[ */
var shutterSettings = {
	msgLoading: "L O A D I N G",
	msgClose: "Click to Close",
	imageCount: "1"
};
/* ]]> */
</script>
<script type='text/javascript' src='http://hci.cs.wisc.edu/aaai10/wp-content/plugins/nextgen-gallery/shutter/shutter-reloaded.js?ver=1.3.0'></script>
<script type='text/javascript' src='http://hci.cs.wisc.edu/aaai10/wp-includes/js/jquery/jquery.js?ver=1.4.2'></script>
<script type='text/javascript' src='http://hci.cs.wisc.edu/aaai10/wp-content/plugins/nextgen-gallery/js/jquery.cycle.all.min.js?ver=2.88'></script>
<script type='text/javascript' src='http://hci.cs.wisc.edu/aaai10/wp-content/plugins/nextgen-gallery/js/ngg.slideshow.min.js?ver=1.03'></script>

<meta name='NextGEN' content='1.7.3' />
<link href='http://fonts.googleapis.com/css?family=Philosopher' rel='stylesheet' type='text/css'>
<link href='http://fonts.googleapis.com/css?family=Cuprum' rel='stylesheet' type='text/css'>
<link href='http://fonts.googleapis.com/css?family=Raleway:100' rel='stylesheet' type='text/css'>
</head>
<body class="custom participants">
<div id="container">
<div id="page">
<ul class="menu">
<li class="tab tab-home"><a href="http://hci.cs.wisc.edu/aaai10">Home</a></li>
<li class="tab tab-1"><a href="http://hci.cs.wisc.edu/aaai10/?page_id=11" title="Organization">Organizers</a></li>
<li class="tab tab-2"><a href="http://hci.cs.wisc.edu/aaai10/?page_id=20" title="Program">Program</a></li>
<li class="tab tab-3"><a href="http://hci.cs.wisc.edu/aaai10/?page_id=169" title="Proceedings">Proceedings</a></li>
<li class="tab tab-4 current"><a href="http://hci.cs.wisc.edu/aaai10/?page_id=160" title="Participants">Participants</a></li>
<li class="tab tab-5"><a href="http://hci.cs.wisc.edu/aaai10/?page_id=171" title="Final Report">Final Report</a></li>
</ul>
	<div id="header">
		<p id="logo"><a href="http://hci.cs.wisc.edu/aaai10">Dialog with Robots</a></p>
		<p id="tagline">AAAI 2010 Fall Symposium</p>
	</div>
	<div id="content_box">
		<div id="content">

			<div class="post_box top" id="post-160">
				<div class="headline_area">
					<h1>Participants</h1>
				</div>
				<div class="format_text">
<p>Below is a list of researchers who participated at the Symposium.</p>
<h2>Marcella Baker</h2>
<p>Affiliation: Carnegie Mellon University</p>
<h2>Cindy L. Bethel</h2>
<p>Affiliation: Yale University</p>
<p>Email: cindy.bethel { at } yale.edu</p>
<p>Web-page: <a href="http://www.cindybethel.com">http://www.cindybethel.com</a></p>
<p>My research interests focus on how intelligent affective agents can recognize and model the affective state of the humans with whom they interact and how those agents can learn and adapt to changes in a human’s affective state during those interactions. Different people have very different emotional responses during social interactions and an intelligent affective agent would have the ability to learn and adaptively respond to those differences. Additionally, there are situations in which an intelligent agent may need to influence a human’s affective states during an interaction in order to facilitate a specific outcome (e.g., facilitating change during therapy).</p>
<h2>Jeannette Bohg</h2>
<p>Affiliation: Computational Vision and Active Perception Lab (CVAP), Centre for Autonomous Systems (CAS), Royal Institute of Technology (KTH), Stockholm, Sweden</p>
<p>Email: bohg { at } csc.kth.se</p>
<p>Web-page: <a href="http://www.csc.kth.se/~bohg">http://www.csc.kth.se/~bohg</a></p>
<p>Before robots can perform everyday task in our household, there is still a long way to go. One requirement for the robot is to have a model of its environment. My research is about giving the robot the ability to actively explore its environment for incrementally building up a scene model. Explorative actions can be either visual, haptic or querying for human assistance. Scientific challenges in this area include the decision about which action is the next best one and how to fuse the multi-modal information into a unified scene model.</p>
<h2>Dan Bohus</h2>
<p>Affiliation: Microsoft Research</p>
<p>Email: dbohus { at } microsoft.com</p>
<p>Web-page: <a href="http://research.microsoft.com/~dbohus">http://research.microsoft.com/~dbohus</a></p>
<p>The central question that drives my long-term research agenda is: how can we develop systems that naturally embed interaction and computation deeply into the flow of everyday tasks, activities and collaborations? More specifically, I am currently pursuing the development of computational models for the core conversational competencies required for supporting seamless spoken dialog interactions in open, dynamic and relatively unconstrained environments. Some of the areas and problems under investigation are: multimodal sensor fusion, conversational scene analysis, models for engagement, turn-taking, interaction planning and grounding in multiparty open-world dialog, as well as lifelong learning and adaptation.</p>
<h2>Cynthia Breazeal</h2>
<p>Affiliation: Massachusetts Institute of Technology</p>
<h2>Dan Brooks</h2>
<p>Affiliation: University of Massachusetts Lowell</p>
<p>Email: dbrooks { at } cs.uml.edu</p>
<p>Webpage: robotics.cs.uml.edu</p>
<p>Research interests in Human Robot Interaction (HRI) and Urban Search and Rescue (USAR</p>
<h2>Joyce Y. Chai</h2>
<p>Affiliation: Department of Computer Science and Engineering, Michigan State University</p>
<p>Email: jchai { at } cse.msu.edu</p>
<p>Web-page: <a href="http://www.cse.msu.edu/~jchai">http://www.cse.msu.edu/~jchai</a></p>
<p>I am broadly interested in context-modeling and discourse processing for human language understanding, multimodal and situated dialogue, and intelligent user interfaces. My recent work includes incorporating non-verbal modalities for word acquisition and language understanding; discourse processing for implicit semantic role labeling, multiparty meeting analysis, and conversation entailment; and multimodal language processing for situated dialogue in both virtual worlds and physical worlds.</p>
<h2>Crystal Chao</h2>
<p>Affiliation: Georgia Institute of Technology</p>
<p>Email: cchao { at } gatech.edu</p>
<p>Web-page: <a href="http://www.cc.gatech.edu/~cchao6/">http://www.cc.gatech.edu/~cchao6/</a></p>
<p>In our lab’s socially guided machine learning scenarios, interactions between a robot learner and a human teacher typically take the form of a structured dialogue. In previous work, I investigated the influence of active learning on learning efficiency as well as interaction balance and fluidity. I also analyzed interaction breakdowns that typically occur in our learning scenarios. The studies highlighted the importance of mediating speaker-listener roles to achieve smooth interaction. This can be accomplished through turn-taking cues such as gaze, gesture, and prosody. Currently I am working on an architecture for controlling such turn-taking cues in a general way.</p>
<h2>Aneesh Chauhan</h2>
<p>Affiliation: Institute of Electronics and Telematics Engineering of Aveiro (IEETA), University of Aveiro</p>
<p>Email: aneesh.chauhan { at } ua.pt</p>
<p>Short research statement</p>
<p>The current focus of my research is on Human-Robot Interaction (HRI) for enabling robots to ground human language. Taking cues from the studies on processes involved in language acquisition in humans (especially children at pre-verbal linguistic stage), I am investigating paradigms that emulate the basic social processes involved in language transfer, where human user have the capacity to teach their vocabulary to a physically embodied robot. I also investigate and have designed a variety of machine learning methods that can assist robots in grounding a vocabulary (through active HRI) in their sensor-based representations.</p>
<h2>Sonia Chernova</h2>
<p>Affiliation: Worcester Polytechnic Institute</p>
<p>Email: soniac { at } wpi.edu</p>
<p>Web-page: <a href="http://users.wpi.edu/~soniac/">http://users.wpi.edu/~soniac/</a></p>
<p>My work focuses on the development of algorithms for natural human-machine interaction that leverage observations of human behavior and social interaction with humans.  I am particularly interested in active learning and online crowdsourcing as a means of training social robots.  My broad research interests span human-robot interaction, adjustable autonomy and machine learning.</p>
<h2>Herbert H. Clark</h2>
<p>Affiliation: Stanford University</p>
<h2>Shinya Fujie</h2>
<p>Affiliation: Waseda University</p>
<h2>Rodolphe Gelin</h2>
<p>Affiliation: Head of Collaborative Projects at Aldebaran Robotics</p>
<p>Email: rgelin { at } aldebaran-robotics.com</p>
<p>Web-page: <a href="http://www.aldebaran-robotics.com">http://www.aldebaran-robotics.com</a></p>
<p>Aldebaran Robotics manufactures and sells Nao, a small humanoid robot. Aldebaran is also involved in several collaborative projects with academic partners. GVLEX, presented in this paper, is one of these. Before joining Aldebaran, in 2008, Rodolphe Gelin has been working for 20 years at the robotic laboratory of CEA, the French Atomic Research Institute.</p>
<h2>Kevin Gold</h2>
<p>Affiliation: Rochester Institute of Technology</p>
<p>Email:   kgold { at } mail.rit.edu</p>
<p>Web-page:  <a href="http://www.informatics.rit.edu/~kcg/index.shtml">http://www.informatics.rit.edu/~kcg/index.shtml</a></p>
<p>Kevin Gold is an Assistant Professor in the department of Interactive Games and Media at RIT. Kevin&#8217;s research broadly explores how agents can infer meaning and intention from the actions of people, using machine learning that is informed by psychology and semantics &#8212; essentially, understanding what people want and mean from what they do and say.</p>
<h2>Javi F. Gorostiza</h2>
<p>Affiliation: Robotics Lab – Carlos III University of Madrid</p>
<p>Email: jgorosti { at } ing.uc3m.es</p>
<p>Web-page: <a href="http://roboticslab.uc3m.es/roboticslab/persona.php?id_pers=18">http://roboticslab.uc3m.es/roboticslab/persona.php?id_pers=18</a></p>
<p>My research activity includes the following topics: Human-Robot Interaction, Dialog Manager Systems, Gesture Generation, Audiovisual Expression, and Natural Robot Programming. I am interested in the different theories of human-human interaction (HHI), that include verbal communication, multimodality management, interaction dynamics, gesture generation, internal models of the world (including the other participant), etc. How do humans learn language? How do they learn interactive skills? How can we enhance interaction process between non-expert humans and social robotics? I work with Social Robots, and I want to implement some aspects of HHI theories in a social robot and test the results with non-expert users.</p>
<h2>Dai Hasegawa</h2>
<p>Affiliation: Hokkaido University, Japan</p>
<p>Email: hasegawadai { at } media.eng.hokudai.ac.jp</p>
<p>Web-page: <a href="http://arakilab.media.eng.hokudai.ac.jp/~hasegawadai/index.html">http://arakilab.media.eng.hokudai.ac.jp/~hasegawadai/index.html</a></p>
<p>I am interested in the development and evaluation of gesture and speech generation mechanism of gesture for robots and ECAs. I am also interested in the development of robots that can perform a casual dialog (not task oriented) while gesturing naturally, due to the fact that one of the problems of current dialog systems is that they are not embodied. A further research interest is the measurement of psychological influences on human attitudes toward embodied systems. Currently, I am examining human feedback following human-robot interaction where the robot shows its ability to acquire language by being taught verb concepts.</p>
<h2>Zachary Henkel</h2>
<p>Affiliation: Texas A&amp;M University</p>
<h2>Eric Horvitz</h2>
<p>Affiliation: Microsoft Research</p>
<h2>Chien-Ming Huang</h2>
<p>Affiliation: University of Wisconsin-Madison</p>
<p>Email: cmhuang { at } cs.wisc.edu</p>
<p>Web-page: <a href="http://www.cs.wisc.edu/~cmhuang">http://www.cs.wisc.edu/~cmhuang</a></p>
<p>My research interests lie in human-robot interaction, intelligent machines, and psychological and cognitive understanding of human behavior and learning. My research goals are to make robotic technology accessible to ordinary people and to leverage intelligently cognitive robots as therapeutic devices for social deficits, assistants for elders and disabled people, and educational partners for children.</p>
<h2>Michita Imai</h2>
<p>Affiliation: Keio University</p>
<h2>Naoto Iwahashi</h2>
<p>Affiliation: National Institute of Information and Communications Technology</p>
<p>Email: naoto.iwahashi { at } nict.go.jp</p>
<p>Web-page:   <a href="http://mastarpj.nict.go.jp/~niwaha/index-e.html">http://mastarpj.nict.go.jp/~niwaha/index-e.html</a></p>
<p>I have been developing online machine learning method L-Core, which enables robots to learn to communicate with human from scratch through verbal and behavioral interaction. L-Core combines sensory-motor information obtained through the interaction, and learns beliefs regarding speech units, words, the concepts of objects, motions, grammar, and pragmatic and communicative capabilities. The overall belief system is represented by a dynamic graphical model. With a small, practical number of learning episodes with human, the robot is eventually able to understand even fragmental and ambiguous utterances, respond to them with confirmation questions and/or actions, generate directive utterances, and answer questions, appropriately for the given situation. I would like to emphasize the importance of a developmental approach to realize personally and physically situated human-robot conversations, in which three requisites in terms of the beliefs &#8211; grounding, scalability, and sharing- should be satisfied.</p>
<h2>David B. Kaber</h2>
<p>Affiliation: Edward P. Fitts Department of Industrial &amp; Systems Engineering</p>
<p>North Carolina State University</p>
<p>Email: dbkaber { at } ncsu.edu</p>
<p>Web-page: <a href="http://www.ise.ncsu.edu/kaber/index.html">http://www.ise.ncsu.edu/kaber/index.html</a></p>
<p>The Ergonomics Lab at NC State has conducted a number of empirical studies of human interaction with robotic systems in simulations of different applications, including: assessing situation awareness and workload effects of multi-modal (visual and auditory) interface design for telerover control in mine disposal tasks; developing computational cognitive models for predicting human behavior in rover navigation control in sparse and stable environmental conditions; and assessing the effects of anthropomorphism in service robots on human emotional responses using objective measures of physiological states. These studies have yielded several masters theses, doctoral dissertations and scholarly publications.</p>
<h2>Takayuki Kanda</h2>
<p>Affiliation: ATR</p>
<p>Email: kanda { at } atr.jp</p>
<p>Web-page: <a href="http://www.irc.atr.jp/~kanda/index.html">http://www.irc.atr.jp/~kanda/index.html</a></p>
<p>I&#8217;m interested in human-robot interaction in a real context, as well as the techniques for robots to naturally interact with people. The former study relates with field trials in various context, such as in school, museum, and shopping mall. The latter include research about techniques to effectively use human-like body properties of robots and techniques to retrieve knowledge and model for robots to fit themselves in a human context.</p>
<h2>John Kelleher</h2>
<p>Affiliation: Applied Intelligence Research Centre, Dublin Institute of Technology, Ireland</p>
<p>Email: john.d.kelleher { at } dit.ie</p>
<p>Web-page: <a href="http:// www.comp.dit.ie/jkelleher">http:// www.comp.dit.ie/jkelleher</a></p>
<p>My main research interest is in the grounding of language in perception. This has led to work in spatial cognition, situated dialog, reference resolution and generation, planning and plan recognition, natural language processing, robotics, and avatars for intelligent agents on smart phones.</p>
<h2>Tetsunori Kobayashi</h2>
<p>Affiliation: Waseda University</p>
<h2>Thomas Kollar</h2>
<p>Affiliation: MIT</p>
<p>Email: tkollar { at } csail.mit.edu</p>
<p>Web-page: <a href="http://tkollar.csail.mit.edu/TK">http://tkollar.csail.mit.edu/TK</a></p>
<p>To be useful teammates to human partners, robots must be able to follow spoken instructions given in natural language. An important class of instructions are those that involve spatial commands and also those that require the robot to interact with people, such as “Follow the person to the kitchen” or “Meet the person at the elevators.” These instructions require both that the robot reason about space and also to act fluidly in order to react to changes in the environment. The approach taken involves learning a cost function that evaluates the quality of a plan with respect to the natural language command and a factored inference that enables plans to be inferred.</p>
<h2>Geert-Jan M. Kruijff</h2>
<p>Affiliation:  German Research Center for Artificial Intelligence, DFKI GmbH</p>
<p>Email:  gj { at } dfki.de</p>
<p>Web-page:  <a href="http://www.dfki.de/~gj">http://www.dfki.de/~gj</a></p>
<p>The key question that I have worked on is how a robot needs to construct contextually-sensitive meaning in the setting of situated dialogue. The line of thought I have followed is that the meaning of situated dialogue is based in a combination of how a robot understands what has been talked about before, and how it can relate dialogue content to how it comprehends the world (experience and indexicality), and what it can and needs to do there (affordances and intentions). Over the years me and my colleagues have worked on approaches for context-sensitive robust spoken dialogue processing, its grounding in various aspects of spatial cognition and visual experience, and its integration in a variety of (mobile) robot systems.</p>
<h2>Rohit Kumar</h2>
<p>Affiliation: Language Technologies Institute, Carnegie Mellon University</p>
<p>Email: rohitk { at } cs.cmu.edu</p>
<p>Web-page: <a href="http://www.rohitkumar.net">http://www.rohitkumar.net</a></p>
<p>Conversational Agents operating in highly dynamic interactive situations such as multi-party interaction must display a variety of interactive behaviors to support / engage the users. I am interested in issues surrounding the identification, implementation, and evaluation of these interactive behaviors. I have developed Basilica, an event-driven software architecture for building agents that facilitates the study of some of these issues. Recent evaluations of the use of social interaction strategies have shown that these behaviors help agents provide better support to students in collaborative learning situations. Currently I am investigating the generalizability of these behaviors and the policy for interleaving social strategies with task related behavior.</p>
<h2>Changsong Liu</h2>
<p>Affiliation: Language and Interaction Research Group, Department of Computer Science and Engineering, Michigan State University</p>
<p>Email: cliu { at } msu.edu</p>
<p>Web-page: <a href="http://www.cse.msu.edu/~cliu/">http://www.cse.msu.edu/~cliu/</a></p>
<p>I am broadly interested in the areas of Human-Computer Interaction, Natural Language Processing and Educational Uses of AI. My current research focuses on Situated Human-Robot Dialogue. I like working on this sub-field because it is interesting and challenging, and advances in this field can also bring valuable insights to other related fields.</p>
<h2>Anne Loomis Thompson</h2>
<p>Affiliation: Microsoft Research</p>
<p>Email: annelo { at } microsoft.com</p>
<p>Web-page: <a href="http://research.microsoft.com/en-us/people/annelo">http://research.microsoft.com/en-us/people/annelo</a></p>
<p>As a research software development engineer, my primary interest is in developing systems that enable natural, open world interaction.  My current projects involve interaction planning for multiparty dialog, multimodal sensor fusion, and integration of predictive systems.  Much of my previous research was focused on robotic manipulation, specifically in the area of multibody dynamics.</p>
<h2>Caroline Lyon</h2>
<p>Affiliation:  University of Hertfordshire, UK</p>
<p>Email: c.m.lyon { at } herts.ac.uk</p>
<p>Web-page: <a href="http://homepages.stca.herts.ac.uk/~comrcml/">http://homepages.stca.herts.ac.uk/~comrcml/</a></p>
<p>My primary interests are in early cognitive developments in humans and their analogs in artificial life. In particular I work on simulating language acquisition in synthetic agents embodied in robots.</p>
<p>Research interests also include the evolution of linguistic communication and the emergence of grammar, using Information Theory techniques to find methods of coding language that make communication more efficient. Previously I developed software for natural language processing, with neural networks and other methods. And I have worked on developing applications for speech recognition technology.</p>
<h2>Brian Mac Namee</h2>
<p>Affiliation: Applied Intelligence Research Centre Dublin Institute of Technology Ireland</p>
<p>Email: Brian.MacNamee { at } dit.ie</p>
<p>Web-page: <a href="http://www.comp.dit.ie/bmacnamee">http://www.comp.dit.ie/bmacnamee</a></p>
<p>My main research interest is in novel applications of machine learning techniques to interesting problems. This has led to work in intelligent virtual agents, robotics, data visualisation, search engines, machine vision, augmented reality, dialog systems and natural language processing. All of this work is driven by a desire to use the power of computational intelligence to make people’s lives better.</p>
<h2>Maxim Makatchev</h2>
<p>Affiliation: PhD student, Robotics Institute, Carnegie Mellon University</p>
<p>Email: mmakatch { at } cs.cmu.edu</p>
<p>Web-page: <a href="http://www.cs.cmu.edu/~mmakatch">http://www.cs.cmu.edu/~mmakatch</a></p>
<p>I am interested in the ways human-human and human-robot dialogue varies between cultures and personalities. I study these issues using social robot receptionists situated in the US and Qatar. My other interests are pragmatics and discourse. Here I will be presenting our work on the pragmatics of questions that users ask the robot receptionists.</p>
<h2>Matthew Marge</h2>
<p>Affiliation: Carnegie Mellon University</p>
<p>Email: mrmarge { at } cs.cmu.edu</p>
<p>Web-page: <a href="http://www.cs.cmu.edu/~mrmarge/">http://www.cs.cmu.edu/~mrmarge/</a></p>
<p>My research interests include spoken dialog systems, human-robot interaction, crowdsourcing for natural language research, and natural language generation. I’m interested in improving human-robot dialogue by better understanding how people use spatial language, both in reference to the environment and to members of a human-robot team. I aim to develop systems that can interpret spatial language from humans fluidly and in turn communicate about space effectively with teammates. My work is in the treasure hunting domain as part of the TeamTalk project.</p>
<h2>Laetitia Matignon</h2>
<p>Affiliation: Post-doctoral researcher at University of Caen Basse-Normandie in MAD (Model, Agent and Decision) team of GREYC laboratory (CNRS UMR 6072), supervised by professor A.I. Mouaddib.</p>
<p>Email: lmatigno { at } info.unicaen.fr</p>
<p>Web-page: <a href="http://lmatigno.perso.info.unicaen.fr/">http://lmatigno.perso.info.unicaen.fr/</a></p>
<p>I am involved in the AMORCES (algorithms and models for a collaborative, eloquent and social robot) NRA (French National Research Agency) project. This project studies decisional and operational human-robot interaction and in particular, the impact of verbal and non-verbal communication on the achievement of collaborative tasks between a robot and its human partner. Especially, I am working on planning and learning in collaborative Human-Robot Interaction. Interests: reinforcement learning, planning, (PO)MDPs, multi-agent systems, human-robot interaction.</p>
<h2>Yoichi Matsuyama</h2>
<p>Affiliation: Department of Computer Science, Waseda University, Japan</p>
<p>Email: matsuyama { at } pcl.cs.waseda.ac.jp</p>
<p>Web-page: <a href="http://www.matsuyama.tv">http://www.matsuyama.tv</a></p>
<p>My Interest lies in the computational models of multiparty conversation/social communication, which combines artificial intelligence, cognitive science and neuroscience. My current PhD project is SCHEMA[she:ma] robot that is oriented to the multiparty conversation, which particularly focused on elderly care tasks.</p>
<h2>Stewart Miller</h2>
<p>Affiliation: University of Illinois at Chicago</p>
<h2>Joseph Modayil</h2>
<p>Affiliation: University of Alberta</p>
<p>Email: jmodayil { at } cs.ualberta.ca</p>
<p>Web-page: <a href="http://webdocs.cs.ualberta.ca/~jmodayil/">http://webdocs.cs.ualberta.ca/~jmodayil/</a></p>
<p>My interests lie in robot learning, focusing on methods to extract useful high-level knowledge from low-level sensorimotor experience. This work includes learning sensor geometry, robot maps, object representations, and activity recognition.  I am currently exploring the idea that communicative intent could be learned from experience.</p>
<h2>Shiwali Mohan</h2>
<p>Affiliation: University of Michigan</p>
<h2>Robin Murphy</h2>
<p>Affiliation: Texas A&amp;M University</p>
<h2>Bilge Mutlu</h2>
<p>Affiliation: University of Wisconsin Madison</p>
<p>Email: bilge { at } cs.wisc.edu</p>
<p>Web-page: <a href="http://pages.cs.wisc.edu/~bilge">http://pages.cs.wisc.edu/~bilge</a>, <a href="http://hci.cs.wisc.edu">http://hci.cs.wisc.edu</a></p>
<p>My research program seeks to gain a computational understanding of human social behavior and to design interactive systems that exhibit on social behavior. Toward gaining a better understanding of social behavior, we develop computational methods, techniques, and tools that facilitate modeling and simulating verbal and nonverbal social cues, communicative mechanisms, and social, cognitive, and emotional processes. We also build applications of socially interactive systems, particularly humanlike agents and robot, that provide social and cognitive benefits in such domains as education, rehabilitation of traumatic and developmental disorders, and understanding of cross-cultural communication.</p>
<h2>Mikio Nakano</h2>
<p>Affiliation: Honda Research Institute Japan Co., Ltd.</p>
<p>Email: nakano { at } jp.honda-ri.com</p>
<p>Web-page: <a href="http://www.jp.honda-ri.com/">http://www.jp.honda-ri.com/</a></p>
<p>My research interests include spoken dialogue interfaces for robots that can work in the physical world. Such interfaces need to understand human utterances and behave depending on the physical situations, such as the locations of robots and humans. They also need to be able to deal with the meanings of linguistic expressions that are grounded on the physical world, and acquire new words and their meanings in the course of natural human-robot dialogues.  I am also interested in spoken language understanding and multi-domain dialogue system architecture for such spoken dialogue interfaces.</p>
<h2>Rodney D. Nielsen</h2>
<p>Affiliation: University of Colorado, Boulder</p>
<p>Email: Rodney.Nielsen { at } Colorado.edu</p>
<p>Web-page:   <a href="http://www.RodneyNielsen.com">http://www.RodneyNielsen.com</a></p>
<p>My primary research interests include computational semantics, natural language processing, machine learning, cognitive science, and the application of these fields to companionbots, clinical informatics, educational technology, and end-user programming.</p>
<h2>Tatsuya Nomura</h2>
<p>Affiliation: Department of Media Informatics, Ryukoku University, Japan</p>
<p>Email: nomura { at } rins.ryukoku.ac.jp</p>
<p>Web-page: <a href="http://www.rikou.ryukoku.ac.jp/~nomura/">http://www.rikou.ryukoku.ac.jp/~nomura/</a></p>
<p>My research interests are on human-robot interaction (HRI), in particular, user studies exploring human factors influencing dialogues with robots like technophobia, investigation of robot behavioral factors affecting human cognition and emotions toward robots, and social impact of robots. For these aims, our research group has developed some psychological scales measuring attitudes and anxiety toward robots. Moreover, our group found that some behavioral factors such as politeness in robots could affect human cognition. In the current situation, we also need to discuss about ethical issues related to HRI such as application areas.</p>
<h2>Sarangi P. Parikh</h2>
<p>Affiliation: Weapons and Systems Engineering, US Naval Academy</p>
<p>Email: sparikh { at } usna.edu</p>
<p>Her research interests include human-robot interaction, human-computer interaction, and interfacing with intelligent technologies.  Currently, she is exploring a number of different interactive devices that may allow for more ‘natural’ interactions between humans and robots.  These devices include Smartphones, larger touch screens such as an iPad, eyeglasses that support augmented reality, and smart gloves.  She is extremely familiar with the Institutional Review Board and has obtained approval for human subjects’ testing for various projects.  Her overall research goal is to enhance communication that humans use when interacting with technology.</p>
<h2>Julia Peltason</h2>
<p>Affiliation: Bielefeld University, Germany</p>
<p>Email: jpeltaso { at } techfak.uni-bielefeld.de</p>
<p>Web-page: <a href="http://aiweb.techfak.uni-bielefeld.de/user/jpeltaso">http://aiweb.techfak.uni-bielefeld.de/user/jpeltaso</a></p>
<p>I am a PhD student working on dialog system engineering for robotics. More specifically, I am developing a dialog framework targeting the demands of advanced human-robot interaction, such as mixed-initiative, multitasking, learning within interaction and situated dialog in dynamic environments. I have proposed an approach to dialog modeling on robots that (1) provides fine-grained interfacing to the robotic subsystem through a task state protocol and (2) encapsulates recurring dialog structures as generic interaction patterns that support rapid prototyping of interaction scenarios. Moreover, I am interested in interaction design and have contributed to a number of scenarios exploring social robots and robot assistants.</p>
<h2>Dennis Perzanowski</h2>
<p>Affiliation: Naval Research Laboratory</p>
<h2>Allison Petrosino</h2>
<p>Affiliation: Wellesley College</p>
<p>Email: allisonpetrosino { at } gmail.com</p>
<p>Allison Petrosino is a recent graduate of Wellesley College, where she was a member of the Gold Intention Recognition Lab.  Allison&#8217;s senior thesis explored how robots might make use of implicit contrast in learning the meanings of adjectives. She was also a neuroscience major and is particularly interested in studying the neurobiology of learning and memory.</p>
<h2>Antoine Raux</h2>
<p>Affiliation: Honda Research Institute USA</p>
<p>Email: araux { at } honda-ri.com</p>
<p>Web-page: <a href="http://sites.google.com/site/antoineraux/">http://sites.google.com/site/antoineraux/</a></p>
<p>With the aim of achieving natural spoken communication with machines in the real world, my research focuses on a number of challenging issues about situated interaction: uncertainty about the environment as well as the user, interaction between dialog and physical actions, asynchronous events and actions… In particular, I believe timely behavior is a critical aspect of such systems, which requires the ability to incrementally process information, make decision, and produce behavior, while constantly updating the system’s beliefs about the world. Currently, I am performing this research using a virtual (PC-based) environment where the world is simulated while the (human-human or human-machine) dialog is real. The next step is to apply the developed algorithms and models to real world robots.</p>
<h2>Wendelin Reich</h2>
<p>Affiliation: Swedish Collegium for Advanced Study</p>
<p>Email: wendelin.reich { at } soc.uu.se</p>
<p>The problem I am working on is that of communicative comprehension. How does one interactant make sense of a verbal message or gesture by a co-interactant? Can we develop a precise, implementable model of this process? These questions have traditionally been studied by psychologists, linguists and AI-researchers working in the Gricean tradition. My approach, in contrast, draws on evolutionary biology, evolutionary psychology and cognitive neuroscience. I suggest that it is fruitful to focus on evolutionarily ancient communicative signals (pointing gestures etc.), and to investigate how these are comprehended in physical face-to-face contexts through embodied cognition and fast-and-frugal computational heuristics.</p>
<h2>Charles Rich</h2>
<p>Affiliation: Worcester Polytechnic Institute</p>
<p>Email: rich { at } wpi.edu</p>
<p>Web-page: <a href="http://www.cs.wpi.edu/~rich">http://www.cs.wpi.edu/~rich</a></p>
<p>My research focuses on collaborative interaction between people and intelligent virtual or robotic agents, including both verbal and nonverbal behaviors (e.g., gaze and pointing) at both the intentional (planning) and engagement (connection events) levels.  I am particularly concerned with building application-independent collaborative dialogue systems, such as Collagen, and its recent open-source successor, Disco.  I have recently started a joint project with Sidner and Bickmore to investigate long-term relationships between people and intelligent agents by placing always-on agents in people&#8217;s homes.  I also do research on artificial intelligence in games.</p>
<h2>Stephanie Rosenthal</h2>
<p>Affiliation: Carnegie Mellon University</p>
<p>Email: srosenth { at } cs.cmu.edu</p>
<p>Web-page: <a href="http://www.cs.cmu.edu/~srosenth">http://www.cs.cmu.edu/~srosenth</a></p>
<p>I&#8217;m interested broadly in the fields of AI planning, human-robot interaction, and human-computer interaction. My work focuses on how humans can help physical devices (robots, mobile devices, etc) reduce their sensor uncertainty and overcome physical limitations in order to perform tasks in the environment more effectively. Specifically, I investigate how agents can model humans to determine and plan for appropriate times to ask for help during task planning and execution.</p>
<h2>Robert Ross</h2>
<p>Affiliation: Applied Intelligence Research Centre, Dublin Institute of Technology, Ireland</p>
<p>Email:  Robert.Ross { at } dit.ie</p>
<p>Web-page: <a href="http://www.robertjross.org">http://www.robertjross.org</a></p>
<p>My main research interest is in the area of situated communication, where I study the means to represent and process spatial and discourse context. I have studied the embodied processing of spatial language, built large-scale ontologies of spatial meaning, and worked with applied robotics platforms such as semi-autonomous wheelchairs. My current research project looks at plan recognition in situated communication.</p>
<h2>Alexander I. Rudnicky</h2>
<p>Affiliation: Carnegie Mellon University</p>
<h2>Niels Schütte</h2>
<p>Affiliation: Dublin Institute of Technology</p>
<h2>Jeremy Searock</h2>
<p>Affiliation: Weapons and Systems Engineering Department, United States Naval Academy</p>
<p>Email: jsearock { at } gmail.com</p>
<p>My applied research focuses on increasing the survivability of autonomous robots and advancing them towards 100% independence through a combination of robust engineering and artificial intelligence.  As a U.S. Naval Submarine Officer, I focus my efforts within the littoral environment with Autonomous Surface and Underwater Vehicles.  Recently, my efforts have focused on classification algorithms to autonomously detect hostile intent.</p>
<h2>Abraham Shultz</h2>
<p>Affiliation:  University of Massachusetts Lowell</p>
<p>Email: ashultz { at } cs.uml.edu</p>
<p>Web-page: <a href="http://robotics.cs.uml.edu">http://robotics.cs.uml.edu</a></p>
<p>Human-Robot Interaction (HRI), rapid prototyping, desktop manufacturing, ubiquitous computing.</p>
<h2>Candy Sidner</h2>
<p>Affiliation:  Worcester Polytechnic Institute</p>
<p>Email: candy.sidner { at } alum.mit.edu</p>
<p>Web-page:   <a href="http://candysidner.com">http://candysidner.com</a></p>
<p>My work is focused on human-machine dialogue and collaboration, especially on agents, virtual robotic.  Dialogue in face-to-face settings is accompanied (in human to human interaction) with many types of non-verbal gestures.  These gestures are critical because they indicate 1) the level of engagement that participants convey to others, 2) social factors such as mutual gaze, 3) express the deictic references of participants, inter alia.  Because the body performs other tasks, producing and interpreting gestures must be integrated with those other uses.  I have recently begun to focus on long-term relationships between people and agents, which will involve deliberation about relationships in collaboration.</p>
<p>Interest keywords:</p>
<p>dialog and collaboration between people and virtual agents and robots; engagement in interactions; non-verbal gestures; prediction and generation of dialogue and gestures; relationships between people and agents</p>
<h2>Ronnie Smith</h2>
<p>Affiliation:  East Carolina University</p>
<h2>Vasant Srinivasan</h2>
<p>Affiliation:  Texas A&amp;M University</p>
<h2>Komei Sugiura</h2>
<p>Affiliation:  National Institute of Information and Communications Technology</p>
<p>Email: komei.sugiura { at } nict.go.jp</p>
<p>Web-page:   <a href="http://mastarpj.nict.go.jp/~ksugiura/index_en.html">http://mastarpj.nict.go.jp/~ksugiura/index_en.html</a></p>
<p>My research interests include robot language acquisition, motion learning, machine learning, cognitive robotics, recommender systems, spoken dialogue systems, and sensor evolution. In particular, I am interested in the application of machine learning techniques to problems in language learning. Since 2008, I have been participating in the RoboCup { at } Home, which is a world-wide benchmarking test for domestic service robots.</p>
<h2>Ming Sun</h2>
<p>Affiliation: Language Technologies Institute, Carnegie Mellon University</p>
<p>Email: mings { at } cs.cmu.edu</p>
<p>I am a first year Master student in the Language Technologies Institute at Carnegie Mellon University. My research interest is human-robot dialogue system. What I am trying to do is  developing the learning ability of the robot to enhance the naturalness of the dialogue. I would like to help the robot raise questions when it encounters any unclear information. The unclear information includes new concept which is out of the robot&#8217;s knowledge or unambiguous information which has more than one corresponding concepts in the robot&#8217;s ontology.</p>
<h2>Manida Swangnetr</h2>
<p>Affiliation: Edward P. Fitts Department of Industrial and Systems Engineering,</p>
<p>North Carolina State University</p>
<p>Email: mswangn { at } ncsu.edu</p>
<p>We have conducted several experiments to identify humanoid features of service robots that lead to positive emotional states for patients in medicine delivery tasks. Results have demonstrated facial features to promote perceived humanness and voice capabilities to contribute to positive emotions. These responses have also been associated with increases in perceived healthcare quality. We have also evaluated methods for patient physiological signal processing as a basis for emotional state classification. Wavelet technology has been use to express cardiac, galvanic skin response and electromyography signals as concise sets of coefficients predicting perceived states of arousal/valence with greater than 75% accuracy.</p>
<h2>Andrea L. Thomaz</h2>
<p>Affiliation:  Georgia Institute of Technology</p>
<p>Email: athomaz { at } cc.gatech.edu</p>
<p>Web-page:  <a href="http://www.cc.gatech.edu/~athomaz/">http://www.cc.gatech.edu/~athomaz/</a></p>
<p>My work focuses on machines that learn new tasks and goals from ordinary people in everyday human environments. This research works from the assumption that machines meant to learn from people can take better advantage of the ways in which people naturally approach teaching. I want to understand and computationally model specific mechanisms of human social learning in order to build machines that participate in social learning interactions. This work has interconnected goals from Artificial Intelligence and Human-Computer/Robot Interaction: improving the performance of a machine’s learning behavior through attention to human interaction and improving the experience of the human teacher by designing interactive learning algorithms based on how people teach. This work tackles many great research questions spanning Machine Learning, Robotics, Human-Computer/Robot Interaction, and Cognitive Science.</p>
<h2>Manuela M. Veloso</h2>
<p>Affiliation:  Computer Science Department, Carnegie Mellon University</p>
<p>Email: veloso { at } cs.cmu.edu</p>
<p>Web-page: <a href="http://www.cs.cmu.edu/~mmv">http://www.cs.cmu.edu/~mmv</a></p>
<p>I research in the area of artificial intelligence and robotics, in particular on the study of agents that Collaborate, Observe, Reason, Act, and Learn, www.cs.cmu.edu/~coral. I investigate planning, execution and learning in teams of autonomous robots, and more recently in mobile companion robots, towards a vision of ubiquitous autonomous mobile robots that can help and coexist with humans.</p>
<h2>Adam Vogel</h2>
<p>Affiliation:  Stanford University</p>
<p>Email address: av { at } cs.stanford.edu</p>
<p>Web-page: <a href="http://cs.stanford.edu/~av/">http://cs.stanford.edu/~av/</a></p>
<p>I&#8217;m interested in connections from language to perception and decision making.  How should robots interpret context-sensitive language in a particular situation, and how does the way they perceive the world influence how they can talk about it?  Particular areas of interest include spatial semantics and the learning of perceptual words such as color terms.</p>
<h2>Holly Yanco</h2>
<p>Affiliation: Computer Science Department, University of Massachusetts Lowell</p>
<p>Email: holly { at } cs.uml.edu</p>
<p>Web-page: <a href="http://www.cs.uml.edu/~holly">http://www.cs.uml.edu/~holly</a>, <a href="http://robotics.cs.uml.edu">http://robotics.cs.uml.edu</a></p>
<p>Research interests include human-robot interaction, multi-touch computing, interface design, robot autonomy, fostering trust of autonomous systems, and evaluation methods. Application domains for this research include assistive technology and urban search and rescue (USAR).</p>
				</div>
			</div>
		</div>

		<div id="sidebars">
			<div id="sidebar_1" class="sidebar">
				<ul class="sidebar_list">
<li class="widget thesis_widget_search" id="search">
<form method="get" class="search_form" action="http://hci.cs.wisc.edu/aaai10/">
	<p>
		<input class="text_input" type="text" value="To search, type and hit enter" name="s" id="s" onfocus="if (this.value == 'To search, type and hit enter') {this.value = '';}" onblur="if (this.value == '') {this.value = 'To search, type and hit enter';}" />
		<input type="hidden" id="searchsubmit" value="Search" />
	</p>
</form>
</li>
<li class="widget widget_text" id="text-4"><h3>Symposium Information</h3>			<div class="textwidget"><p>The "Dialog with Robots" Symposium was held as a part of the <a href="http://www.aaai.org/Symposia/Fall/fss10.php">AAAI 2010 Fall Symposia</a> November 11–13 in Arlington, VA. The Symposium attracted 43 contributed papers and more than 70 participants.</p>
<p>The <a href="http://hci.cs.wisc.edu/aaai10/?page_id=169">proceedings</a>, <a href="http://hci.cs.wisc.edu/aaai10/?page_id=20">program</a>, and the list of <a href="http://hci.cs.wisc.edu/aaai10/?page_id=160">participants</a> of the Symposium as well as the slides for the <a href="http://hci.cs.wisc.edu/aaai10/wp-content/uploads/2010/OpeningDiscussion.pdf">opening</a> and <a href="http://hci.cs.wisc.edu/aaai10/wp-content/uploads/2010/ClosingDiscussion.pdf">closing</a> discussions are available on this website.</p>
</div>
		</li><li class="widget widget_slideshow" id="slideshow-3"><h3>Symposium Photos</h3>		<div class="ngg_slideshow widget">
			<div id="ngg-slideshow-0-160-1" class="ngg-widget-slideshow" style="height:180px;width:240px;">
<div id="ngg-slideshow-0-160-1-loader" class="ngg-slideshow-loader" style="height:180px;width:240px;">
<img src="http://hci.cs.wisc.edu/aaai10/wp-content/plugins/nextgen-gallery/images/loader.gif" alt="" />
</div></div>

<script type="text/javascript" defer="defer">
jQuery(document).ready(function(){ 
jQuery("#ngg-slideshow-0-160-1").nggSlideshow( {id: 0,fx:"fade",width:240,height:180,domain: "http://hci.cs.wisc.edu/aaai10/",timeout:6000});
});
</script>		</div>
		</li><li class="widget widget_text" id="text-3"><h3>Organizers</h3>			<div class="textwidget"><p><a href="http://research.microsoft.com/en-us/um/people/dbohus/">Dan Bohus</a><br />
<i>Microsoft Research</i></p>
<p><a href="http://research.microsoft.com/en-us/um/people/horvitz/">Eric Horvitz</a><br />
<i>Microsoft Research</i></p>
<p><a href="http://www.irc.atr.jp/~kanda/">Takayuki Kanda</a><br />
<i>ATR</i></p>
<p><a href="http://bilgemutlu.com/">Bilge Mutlu</a><br />
<i>University of Wisconsin-Madison</i></p>
<p><a href="http://www.linkedin.com/in/antoineraux">Antoine Raux</a><br />
<i>Honda Research Institute USA</i></p></div>
		</li>				</ul>
			</div>
		</div>
	</div>
	<div id="footer">
    <p>Copyright &copy 2010. <a href="http://hci.cs.wisc.edu/aaai10">Dialog with Robots</a>. All rights reserved. || <a href="http://hci.cs.wisc.edu/aaai10/wp-admin/"<strong>Log-in</strong></a></p><br/>
	</div>
</div>
</div>
<!--[if lte IE 8]>
<div id="ie_clear"></div>
<![endif]-->
</body>
</html>