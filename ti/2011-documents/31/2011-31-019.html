<html>
<title>Crowdsourcing for relevance evaluation and related applications</title>
<body>
<h3>Crowdsourcing for relevance evaluation</h3>
I am interested in leveraging crowd knowledge for a wide range of problems.
Mostly working with Amazon Mechanical Turk and trying all sorts of things for
relevance (TREC and INEX), document summarization, and related stuff.
<p>
Book:
<ul>
 <li>O. Alonso, G. Kazai, and S. Mizzaro. Crowdsourcing for search engine evaluation, Springer 2011. Table of contents coming soon.
 </li>
</ul>
<p>
Tutorial and talks:
<ul>
 <li>Design of experiments for crowdsourcing search evaluations: challenges and opportunities. <a href="http://www.ischool.utexas.edu/~cse2010/program.htm">SIGIR 2010 Workshop</a>.
 </li>
 <li>
<a href="http://kmi.open.ac.uk/events/ecir2010/workshops-tutorials.php">
Crowdsourcing for Relevance Evaluation</a> at <a href="http://kmi.open.ac.uk/events/ecir2010/">ECIR 2010</a>.
Email me if you would like a copy of the slides.
 </li>
 <li>
 <a href="http://mturkpublic.s3.amazonaws.com/DocumentShare/OmarAlonsoMTurk_meetup052010.pdf">Design of experiments and other practical tips</a>. Slides from the Mechanical 
Turk meetup in Palo Alto, May 2010.
 </li>
</ul>
<p>
Publications:
<ul>
<li>A TREC Analysis using Crowdsourcing. In progress</li>
<li>Detecting Uninteresting Content in Text Streams, SIGIR Crowdsourcing for Search Evaluation Workshop, 2010.</li>
<li>An Analysis of Crowdsourcing Relevance Assessments in Spanish, CERI 2010.</li>
<li>Crowdsourcing Assessments for XML Ranked Retrieval, ECIR 2010.</li>
 <li>Relevance Criteria for E-Commerce: A Crowdsourcing-based Experimental 
Analysis (poster), SIGIR 2009.</li>
 <li>Can we get rid of TREC assessors? Using Mechanical Turk for relevance asses
sment, SIGIR IR Evaluation Workshop 2009. (<a href="http://staff.science.uva.nl/
%7Ekamps/ireval/papers/paper_22.pdf">pdf</a>)</li>
 <li>Guidelines for Designing Crowdsourcing-based Relevance
     Experiments (<a href="ExperimentDesign.pdf">pdf</a>). Mainly tips for
     Mechanical Turk. Full-paper coming soon. 
 </li>
 <li>
<a href="http://sigir.org/forum/2008D/papers/2008d_sigirforum_alonso.pdf">
Crowdsourcing for relevance evaluation</a>, SIGIR Forum, December 2008.
</li>
</ul>

<a href="http://wwwcsif.cs.ucdavis.edu/~alonsoom">Back</a>
<hr>

<script src="http://www.google-analytics.com/urchin.js" type="text/javascript">
</script>
<script type="text/javascript">
_uacct = "UA-1383291-1";
urchinTracker();
</script>

<script type="text/javascript">
var sc_project=5741361; 
var sc_invisible=1; 
var sc_partition=64; 
var sc_click_stat=1; 
var sc_security="4da75d9c"; 
</script>

<script type="text/javascript"
src="http://www.statcounter.com/counter/counter.js"></script><noscript><div
class="statcounter"><a title="visit counter for blogspot"
href="http://www.statcounter.com/blogger/"
target="_blank"><img class="statcounter"
src="http://c.statcounter.com/5741361/0/4da75d9c/1/"
alt="visit counter for blogspot" ></a></div></noscript>


</body>
</html>
